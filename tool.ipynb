{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content\n",
    "* [Extract DML PIX info](#section1)\n",
    "* [EXtract DML DDI info](#section2)\n",
    "* [Extract OpenVINO performance data](#section3)\n",
    "* [Extract onnxruntime profiling data](#section4)\n",
    "* [DML&OV Layer By Layer profiling](#section5)\n",
    "> tip1: the links above don't work in browser and only works in locak dev. Please seach the key words in browser to locate the position.\n",
    "\n",
    "> tip2: sometimes the scripts below may need some modifications to perfectly match your own requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference onnx model with DML \n",
    "> related packages need to be installed, e.g. `onnxruntime-directml,onnx,skl2onnx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import onnx\n",
    "model_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\model_rename\\Adobe-Lr_dn_bayerlinear1024.onnx\"\n",
    "#model_path = \"test.onnx\"\n",
    "model = onnx.load(model_path)   \n",
    "data_dic = {}\n",
    "def switch_case(input_datatype):\n",
    "    if input_datatype == \"FLOAT16\":\n",
    "        return np.float16\n",
    "    if input_datatype == \"FLOAT32\":\n",
    "        return np.float32\n",
    "for _input in model.graph.input:\n",
    "    input_name = _input.name\n",
    "    input_datatype = onnx.TensorProto.DataType.Name( _input.type.tensor_type.elem_type)\n",
    "    input_shape = [int(dim.dim_value) for dim in _input.type.tensor_type.shape.dim] \n",
    "    np_dataype = switch_case(input_datatype)\n",
    "    input_data = np.random.randn(*input_shape).astype(np_dataype) # The * before the shape variable is used to unpack the values from the shape list \n",
    "    data_dic[input_name] = input_data\n",
    "\n",
    "sess_options = ort.SessionOptions()\n",
    "dml_provider = [\"DmlExecutionProvider\"]\n",
    "ref_provider = [\"CPUExecutionProvider\"]\n",
    "\n",
    "sess = ort.InferenceSession(model_path, sess_options=sess_options,providers=dml_provider)\n",
    "output_name = sess.get_outputs()[0].name\n",
    "\n",
    "dml_output = sess.run([output_name], data_dic)\n",
    "print(dml_output)\n",
    "\n",
    "sess = ort.InferenceSession(model_path, sess_options=sess_options,providers=ref_provider)\n",
    "ref_output = sess.run([output_name], data_dic)\n",
    "print(ref_output)\n",
    "\n",
    "# you can compare the dml_output and ref_output manually but there will be still small gaps because the precision difference beween GPU and CPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the output in middle layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "import os\n",
    "import onnx\n",
    "from onnx import helper\n",
    "\n",
    "model_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\model_rename\\Adobe-Lr_dn_bayerlinear1024.onnx\"\n",
    "#######################################################################\n",
    "'''\n",
    "ref: https://github.com/microsoft/onnxruntime/issues/1455#issuecomment-514805365\n",
    "This is ONNX debug\n",
    "The purpose is to inspect the desired intermediate layer from onnx\n",
    "\n",
    "Flow:\n",
    "    - A. Load the model and do some sanity check on it\n",
    "    - A*. You could inspect the input/output here\n",
    "    - B. Get the model node (length and node names in string)\n",
    "        *Add this step, you could use any editor, and CTRL+F your desired layer\n",
    "        in my case, I want to get Resnet101 with block4 output\n",
    "    - C. Put the name into some variables\n",
    "    - D. Add the intermediate layers' name into the helper function\n",
    "    - E. Add the helper function into model output\n",
    "    - F. Save the model with the added outputs\n",
    "    - G. Load the customized model\n",
    "    - H. Inspect the new I/O's names of the customized model here\n",
    "    - I. Do some inference on your image, and get/save the output\n",
    "    \n",
    "'''\n",
    "\n",
    "# Step A\n",
    "# load and check that the model is correct or not\n",
    "model = onnx.load(model_path)\n",
    "#onnx.checker.check_model(model)\n",
    "\n",
    "# Step A*\n",
    "#sess = rt.InferenceSession(os.path.join(path,'frozen.onnx'))\n",
    "##sess = rt.InferenceSession('D:\\\\embedmask_simplifier.onnx')\n",
    "#input1 = sess.get_inputs()[0].name\n",
    "#outputs = sess.run([], {input1: img_preprocess})\n",
    "\n",
    "# Step B\n",
    "# get the output of block 4 add and relu operation for each add layer\n",
    "node_count = len(model.graph.node)\n",
    "model_node = str(model.graph.node)\n",
    "\n",
    "# Step C\n",
    "# find the name first\n",
    "unit1_name = '/model/model/encoders.0/encoders.0.0/act/LeakyRelu_output_0'\n",
    "unit2_name = '/model/model/encoders.0/encoders.0.0/conv2/Conv_output_0'\n",
    "\n",
    "# Step D\n",
    "# add into the current onnx model and save to a new model\n",
    "info_unit1_name = helper.ValueInfoProto()\n",
    "info_unit1_name.name = unit1_name\n",
    "info_unit2_name = helper.ValueInfoProto()\n",
    "info_unit2_name.name = unit2_name\n",
    "\n",
    "# Step E\n",
    "model.graph.output.extend([info_unit1_name,info_unit2_name])\n",
    "\n",
    "# Step F\n",
    "onnx.save(model, os.path.join('frozen_out.onnx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save ONNX subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx.helpers.onnx_helper import select_model_inputs_outputs\n",
    "from skl2onnx.helpers.onnx_helper import save_onnx_model\n",
    "from skl2onnx.helpers.onnx_helper import enumerate_model_node_outputs\n",
    "from skl2onnx.helpers.onnx_helper import load_onnx_model\n",
    "model_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\model_rename\\Adobe-Lr_dn_bayerlinear1024.onnx\"\n",
    "model_onnx = load_onnx_model(model_path)\n",
    "# for out in enumerate_model_node_outputs(model_onnx):\n",
    "#     print(out)\n",
    "\n",
    "\"\"\"\n",
    "/model/model/intro/Conv_output_0\n",
    "/model/model/encoders.0/encoders.0.0/conv1/Conv_output_0\n",
    "/model/model/encoders.0/encoders.0.0/act/LeakyRelu_output_0\n",
    "/model/model/encoders.0/encoders.0.0/conv2/Conv_output_0\n",
    "/model/model/encoders.0/encoders.0.0/Mul_output_0\n",
    "/model/model/encoders.0/encoders.0.0/Add_output_0\n",
    "/model/model/encoders.0/encoders.0.0/conv3/Conv_output_0\n",
    "/model/model/encoders.0/encoders.0.0/act_1/LeakyRelu_output_0\n",
    "/model/model/encoders.0/encoders.0.0/conv4/Conv_output_0\n",
    "/model/model/encoders.0/encoders.0.0/Mul_1_output_0\n",
    "/model/model/encoders.0/encoders.0.0/Add_1_output_0\n",
    "/model/model/downs.0/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.0/conv1/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.0/act/LeakyRelu_output_0\n",
    "/model/model/encoders.1/encoders.1.0/conv2/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.0/Mul_output_0\n",
    "/model/model/encoders.1/encoders.1.0/Add_output_0\n",
    "/model/model/encoders.1/encoders.1.0/conv3/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.0/act_1/LeakyRelu_output_0\n",
    "/model/model/encoders.1/encoders.1.0/conv4/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.0/Mul_1_output_0\n",
    "/model/model/encoders.1/encoders.1.0/Add_1_output_0\n",
    "/model/model/encoders.1/encoders.1.1/conv1/Conv_output_0\n",
    "/model/model/encoders.1/encoders.1.1/act/LeakyRelu_output_0\n",
    "/model/model/encoders.1/encoders.1.1/conv2/Conv_output_0\n",
    "...\n",
    "/model/model/ups.2/ups.2.0/Resize_output_0\n",
    "/model/model/ups.3/ups.3.0/Resize_input_cast_0\n",
    "/model/model/ups.3/ups.3.0/Resize_input_cast_2\n",
    "/model/model/ups.3/ups.3.0/Resize_output_\n",
    "\"\"\"\n",
    "\n",
    "num_onnx = select_model_inputs_outputs(model_onnx, '/model/model/encoders.0/encoders.0.0/conv3/Conv_output_0')\n",
    "save_onnx_model(num_onnx, \"test2.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Adobe model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = \"model_mapping.txt\"\n",
    "line_count = 0\n",
    "dict_name={}\n",
    "with open(mapping_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line_count+=1\n",
    "        if line_count == 1:\n",
    "            continue\n",
    "        a,b=line.rstrip().split(\"\t\")\n",
    "        dict_name[b] = a\n",
    "\n",
    "print(dict_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all onnx model\n",
    "import os\n",
    "import shutil\n",
    "# Provide the directory path\n",
    "directory = r\"Adobe_models\\Adobe_models\\AI_Model_Restricted\"\n",
    "new_directory = r\"model_rename\"\n",
    "# Traverse through the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    \n",
    "    for file in files:\n",
    "        # Print the file name\n",
    "        key = file.split(\".onnx\")[0] \n",
    "        if key in dict_name.keys():\n",
    "\n",
    "            target_path = os.path.join(new_directory, dict_name[key]+\".onnx\")\n",
    "            source_path = os.path.join(root,file)\n",
    "            if not os.path.exists(source_path):\n",
    "                print(source_path)\n",
    "                continue\n",
    "            shutil.copy2(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='section2'>Extract DML DDI info </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "ddi_root= r\"C:\\Intel\\igfx\\d3d12\"\n",
    "new_root= r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_backup\"\n",
    "modelfile = glob.glob(r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\model_rename\\*.onnx\")\n",
    "#print(modelfile)\n",
    "for i in range(45):\n",
    "        ddi_file = str(i+1)+\".log\"\n",
    "        print(os.path.basename(modelfile[i]))\n",
    "        modelname = os.path.basename(modelfile[i]).split(\".onnx\")[0]\n",
    "        source_path = os.path.join(ddi_root, ddi_file)\n",
    "        target_path = os.path.join(new_root, modelname+\".log\")\n",
    "        if not os.path.exists(source_path):\n",
    "            print(source_path)\n",
    "            continue\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "input_string = \"Input = [0x1,0x200,0x80,0x80], Filter = [0x200,0x200,0x3,0x3], Bias = [0x1,0x200,0x1,0x1], Output = [0x1,0x200,0x80,0x80]\"\n",
    "input_string = \"Input = [0x1,0x3,0x400,0x400], Filter = [0x80,0x3,0x3,0x3], Bias = [0x1,0x80,0x1,0x1], Output = [0x1,0x80,0x400,0x400]\"\n",
    "values = re.findall(r'0x\\w+', input_string)\n",
    "output = []\n",
    "for idx, val in enumerate(values):\n",
    "    output.append(int(val, 16))\n",
    "\n",
    "print(\"input \", output[:4])\n",
    "print(\"filter\", output[4:8])\n",
    "print(\"bias  \", output[8:12])\n",
    "print(\"output\", output[12:16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ddi log into json file\n",
    "> *todo*: sometimes, the kernel is failed, but the related kernel information will be still dumped in the log. Need to be handled during parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_ddiConv_json(root_path,filename):\n",
    "    ddi_file = os.path.join(root_path, filename)\n",
    "    json_file = os.path.join(root_path, filename.replace(\"log\",\"_conv.json\"))\n",
    "    dic ={}\n",
    "    openf =  open(ddi_file, \"r\")\n",
    "\n",
    "    lines = openf.readlines()\n",
    "    #print(len(lines))\n",
    "    i = -1\n",
    "    while i < len(lines)-1:\n",
    "        i+=1\n",
    "        if \"Passed-Metacommand type : Convolution1\" in lines[i]:\n",
    "            #print(lines[i-12:i])\n",
    "            mc_type = lines[i].rstrip().split(\"type :\")[-1]\n",
    "            \n",
    "            kernel_name_idx = -1\n",
    "            for k in range(11):\n",
    "                if \"Conv Kernel:\" in lines[i-k] \\\n",
    "                    or \"Gemm Kernel:\" in lines[i-k]:  # for the case of GemmBasedConvolution \n",
    "                    kernel_name_idx = i-k\n",
    "                    # print(lines[kernel_name_idx])\n",
    "                    # print(kernel_name_idx)\n",
    "                    break\n",
    "                if k == 10:\n",
    "                    assert(\"no conv kernel\")\n",
    "                    \n",
    "            kernel_name = lines[kernel_name_idx].rstrip().split(\"Kernel:\")[-1]\n",
    "            #print(mc_type, kernel_name)\n",
    "            if mc_type not in dic.keys():\n",
    "                dic[mc_type] = {}\n",
    "            if kernel_name not in dic[mc_type].keys():\n",
    "                dic[mc_type][kernel_name] = []\n",
    "            \n",
    "            layout_idx = -1\n",
    "            dim_idx = -1\n",
    "            param_str = []\n",
    "            for k in range(8):\n",
    "                dim_idx = kernel_name_idx-k\n",
    "                layout_idx = kernel_name_idx - k+2\n",
    "                param_str =  re.findall(r'0x\\w+', lines[dim_idx])\n",
    "                if len(param_str) > 5 \\\n",
    "                    and \"Filter\" in lines[dim_idx]:  # for the case of GemmBasedConvolution\n",
    "                    break\n",
    "\n",
    "            # print(dim_idx, lines[dim_idx])   \n",
    "            # print(param_str)\n",
    "            param_list = []\n",
    "            for idx, val in enumerate(param_str):\n",
    "                param_list.append(int(val, 16))\n",
    "            \n",
    "            layout_str = \"\"\n",
    "            #print(lines[layout_idx].rstrip(), layout_idx,kernel_name_idx)\n",
    "            layout_str = re.findall(r'N\\w+', lines[layout_idx])\n",
    "            #print(kernel_name_idx, lines[dim_idx], lines[layout_idx])\n",
    "\n",
    "            ''' some log  BiasDesc = isNull'''\n",
    "            output_shape = []\n",
    "            if \"IsNull\" in lines[i+5+38]:\n",
    "                bias_value = \"isnull\"\n",
    "                outputdesc_idx = i+5+38+2\n",
    "                output_shape = param_list[9:13]\n",
    "            else:\n",
    "                outputdesc_idx = i+5+19+38\n",
    "                bias_value = param_list[9:13]\n",
    "                output_shape =  param_list[13:17]\n",
    "\n",
    "            input_stride = []\n",
    "            for j in range(4):\n",
    "                input_stride.append(int(lines[i+12+j].rstrip().split(\"=\")[-1],16))\n",
    "            output_stride = []\n",
    "            for j in range(4):\n",
    "                output_stride.append(int(lines[outputdesc_idx+8+j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            # inputpadding_list = \n",
    "            outputpadding_list = []\n",
    "            for j in range(5):\n",
    "                outputpadding_list.append(lines[outputdesc_idx+35+j].rstrip().split(\"=\")[-1])\n",
    " \n",
    "            filter_stride = []\n",
    "            for j in range(3):\n",
    "                filter_stride.append(int(lines[outputdesc_idx+22+j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            filter_dilation =  []\n",
    "            for j in range(3):\n",
    "                filter_dilation.append(int(lines[outputdesc_idx+25+j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "            input_padding = []\n",
    "            for j in range(6):\n",
    "                input_padding.append(int(lines[outputdesc_idx+28+j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "\n",
    "            \n",
    "            group_count =int( re.findall(r'0x\\w+', lines[outputdesc_idx+40])[0],16)\n",
    "            \n",
    "            info_dic ={}\n",
    "            info_dic[\"input\"] = {\"shape\": param_list[1:5], \"layout\": layout_str[0], \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[i+6])[0], \"stride\": input_stride, \"padding\": input_padding}\n",
    "            info_dic[\"filter\"] = {\"shape\": param_list[5:9], \"layout\": layout_str[1], \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5 + 19])[0],\"flag\": re.findall(r'\\((.*?)\\)', lines[i+6+19])[0],\"stirde\": filter_stride, \"dilation\":filter_dilation,  \"groupcount\": group_count}\n",
    "            info_dic[\"output\"] = {\"shape\":output_shape, \"layout\": layout_str[2], \"datatype\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+1])[0],\"flag\":re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+2])[0], \"stride\": output_stride,\"padding\":outputpadding_list}\n",
    "            info_dic[\"bias\"] = bias_value\n",
    "            info_dic[\"direction\"] = re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+20])[0]\n",
    "            \n",
    "            exec_flag_idx = 0\n",
    "            if \"Function\" in lines[outputdesc_idx+42]:\n",
    "                info_dic[\"activation\"] =  re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+42])[0]\n",
    "                exec_flag_idx = outputdesc_idx+46\n",
    "            else:\n",
    "                info_dic[\"activation\"] = \"isnull\"\n",
    "                exec_flag_idx = outputdesc_idx+44\n",
    "            \n",
    "            exec_flag_info = re.findall(r'0x\\w+', lines[exec_flag_idx])\n",
    "            if len(exec_flag_info) > 0:\n",
    "                info_dic[\"exec_flag\"] = int( exec_flag_info[0],16)\n",
    "            else:\n",
    "                info_dic[\"exec_flag\"] = 0\n",
    "                \n",
    "            dic[mc_type][kernel_name].append(info_dic)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    json_object = json.dumps(dic, indent=4)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_ddiGEMM_json(root_path, basename):\n",
    "    ddi_file = os.path.join(root_path, basename)\n",
    "    json_file = os.path.join(root_path, basename.replace(\".log\",\"_gemm.json\"))\n",
    "    dic ={}\n",
    "    openf =  open(ddi_file, \"r\")\n",
    "\n",
    "    lines = openf.readlines()\n",
    "    #print(len(lines))\n",
    "    i = -1\n",
    "    while i < len(lines)-1:\n",
    "        i+=1\n",
    "        if \"Passed-Metacommand type : GEMM1\" in lines[i]:\n",
    "            info_dic ={}\n",
    "            mc_type = lines[i].rstrip().split(\"type :\")[-1]\n",
    "            kernel_name_idx = -1\n",
    "            kernel_name = \"\"\n",
    "            for k in range(10):\n",
    "                if \"Gemm Kernel:\" in lines[i-k]:\n",
    "                    kernel_name_idx = i-k\n",
    "                    kernel_name = lines[kernel_name_idx].rstrip().split(\"Kernel:\")[-1]\n",
    "                    break\n",
    "                if k == 8:  # max search = 6 if not using cache shader ; max search = 6+ 1 if using cache shader\n",
    "                    kernel_name_idx = i-4\n",
    "                    kernel_name = \"unknown\"\n",
    "                    print(\"no gemm kernel\")\n",
    "                    assert(False)\n",
    "\n",
    "            #print(mc_type, kernel_name)\n",
    "            if mc_type not in dic.keys():\n",
    "                dic[mc_type] = {}\n",
    "            if kernel_name not in dic[mc_type].keys():\n",
    "                dic[mc_type][kernel_name] = []\n",
    "            \n",
    "            dim_idx = kernel_name_idx - 2\n",
    "            param_str =  re.findall(r'0x\\w+', lines[dim_idx])\n",
    "            if len(param_str) < 10:\n",
    "                assert(False)\n",
    "\n",
    "            param_list = []\n",
    "            for idx, val in enumerate(param_str):\n",
    "                param_list.append(int(val, 16))\n",
    "\n",
    "            inputA_shape = param_list[3:5] # this first value is for thread id, gemm only has two dim MxK\n",
    "            inputB_shape = param_list[7:9] # K x N\n",
    "            \n",
    "            \n",
    "            outputdesc_idx = 0\n",
    "            if \"IsNull\" in lines[i + 41+ 2]:\n",
    "                outputdesc_idx = i + 41 + 4 # 4 is start from Cdes\n",
    "                info_dic[\"inputC\"] = \"isnull\"\n",
    "                output_shape = param_list[11:13]\n",
    "            else:\n",
    "                # Cdes is not null\n",
    "                outputdesc_idx = i + 41 + 21\n",
    "                info_dic[\"inputC\"] = \"not null\"\n",
    "                output_shape = param_list[15:17]\n",
    "\n",
    "            '''stride is useless '''\n",
    "            # inputA_stride = []\n",
    "            # for j in range(4):\n",
    "            #     inputA_stride.append(int(lines[i+12+j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "            # inputB_stride = []\n",
    "            # for j in range(4):\n",
    "            #     inputB_stride.append(int(lines[i+31+j].rstrip().split(\"=\")[-1],16))\n",
    "            #     #print(lines[i+31+j])\n",
    "\n",
    "            # output_stride = []\n",
    "            # for j in range(4):\n",
    "            #     output_stride.append(int(lines[outputdesc_idx +  8 +j].rstrip().split(\"=\")[-1],16))\n",
    "            #     #print(lines[outputdesc_idx +  8 +j])\n",
    "            \n",
    "            transA = \"false\" if int(lines[outputdesc_idx +  20].rstrip().split(\"=\")[-1],16) == 0 else \"true\"\n",
    "            transB = \"false\" if int(lines[outputdesc_idx +  21].rstrip().split(\"=\")[-1],16) ==0 else \"true\"\n",
    "            alpha = lines[outputdesc_idx +  22].rstrip().split(\"=\")[-1]\n",
    "            beta = lines[outputdesc_idx +  23].rstrip().split(\"=\")[-1]\n",
    "\n",
    "            info_dic[\"config\"] = {\"alpha\": alpha, \"beta\": beta}\n",
    "            info_dic[\"inputA\"] = {\"shape\": inputA_shape,  \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[i+6])[0], \"transA\": transA}\n",
    "            info_dic[\"inputB\"] = {\"shape\": inputB_shape,  \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5+19])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[i+6 + 19])[0], \"transB\": transB}       \n",
    "            info_dic[\"output\"] = {\"shape\":output_shape,   \"datatype\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx + 1])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx + 2])[0]}\n",
    "            \n",
    "            dic[mc_type][kernel_name].append(info_dic)   \n",
    "\n",
    "    json_object = json.dumps(dic, indent=4)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_ddiPool_json(root_path, basename):\n",
    "    ddi_file = os.path.join(root_path, basename)\n",
    "    json_file = os.path.join(root_path, basename.replace(\".log\",\"_pool.json\"))\n",
    "    dic ={}\n",
    "    openf =  open(ddi_file, \"r\")\n",
    "\n",
    "    lines = openf.readlines()\n",
    "    #print(len(lines))\n",
    "    i = -1\n",
    "    while i < len(lines)-1:\n",
    "        i+=1\n",
    "        if \"Passed-Metacommand type : Pooling\" in lines[i]:\n",
    "            info_dic ={}\n",
    "            mc_type = lines[i].rstrip().split(\"type :\")[-1]\n",
    "            kernel_name_idx = i - 3\n",
    "\n",
    "            kernel_name = lines[kernel_name_idx].rstrip().split(\"Kernel:\")[-1] \n",
    "            kernel_name = \"unknown\" if kernel_name==\"\" else kernel_name\n",
    "            #print(mc_type, kernel_name)\n",
    "            if mc_type not in dic.keys():\n",
    "                dic[mc_type] = {}\n",
    "            if kernel_name not in dic[mc_type].keys():\n",
    "                dic[mc_type][kernel_name] = []      \n",
    "\n",
    "           \n",
    "            layout_list = re.findall(r'N\\w+', lines[i+4]) \n",
    "\n",
    "            input_shape = []\n",
    "            for j in range(4):\n",
    "                input_shape.append(int(lines[i+9+j].rstrip().split(\"=\")[-1],16))\n",
    "            '''offset is useless'''\n",
    "            # input_stride = []\n",
    "            # for j in range(4):\n",
    "            #     input_stride.append(int(lines[i+13+j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            outputdesc_idx = i+24\n",
    "            output_shape = []\n",
    "            for j in range(4):\n",
    "                output_shape.append(int(lines[outputdesc_idx + 4 + j].rstrip().split(\"=\")[-1],16))\n",
    "            '''offset is useless'''\n",
    "            # output_stride = []\n",
    "            # for j in range(4):\n",
    "            #     output_stride.append(int(lines[outputdesc_idx +  8 +j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "            available_function = [\"AvgPool\", \"L2Pool\", \"MaxPool\"]  # ref from driver code MetaCommandPoolingDnnl line62\n",
    "            pooling_function = available_function[int(lines[outputdesc_idx + 19].rstrip().split(\"=\")[-1],16)]\n",
    "            stride = []\n",
    "            for j in range(2):\n",
    "                stride.append(int(lines[outputdesc_idx +  21 +j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "            kernel_size = []  #WindowSize\n",
    "            for j in range(2):\n",
    "                kernel_size.append(int(lines[outputdesc_idx +  24 +j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            padding_size = []  #[h_begin, w_begin, h_end, w_end ]\n",
    "            for j in range(6):\n",
    "                if j == 2 or j ==5:\n",
    "                    continue\n",
    "                padding_size.append(int(lines[outputdesc_idx +  27 +j].rstrip().split(\"=\")[-1],16))    \n",
    "\n",
    "            '''m_PoolingParams.PoolingType is not kernel size'''\n",
    "            info_dic[\"input\"] = {\"layout\": layout_list[0],\"shape\": input_shape,  \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+6])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[i+7])[0]}\n",
    "            info_dic[\"filter\"] = {\"shape\": kernel_size, \"stride\": stride, \"padding\": padding_size}\n",
    "            info_dic[\"pool_function\"] = pooling_function\n",
    "            info_dic[\"output\"] = {\"layout\": layout_list[1], \"shape\":output_shape,   \"datatype\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx + 1])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx + 2])[0]}\n",
    "            \n",
    "            dic[mc_type][kernel_name].append(info_dic)   \n",
    "\n",
    "    json_object = json.dumps(dic, indent=4)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "def create_ddiMHA_csv( root_path,ddi_file):\n",
    "\n",
    "    openf =  open(os.path.join(root_path,ddi_file), \"r\")\n",
    "    lines = openf.readlines()\n",
    "\n",
    "    i = -1\n",
    "    csv_file = os.path.join(root_path,\"DDI_MHA.csv\")\n",
    "    csvf = open(csv_file,\"w\",newline='')\n",
    "    writer =csv.writer(csvf)\n",
    "\n",
    "    line = [\"mc_type\",\"gemm0_kernel_name\", \"gemm0_A_size\", \"gemm0_B_size\",\"gemm0_Output_size\",\n",
    "            \"gemm1_kernel_name\", \"gemm1_A_size\", \"gemm1_B_size\",\"gemm1_Output_size\"]\n",
    "    writer.writerow(line)\n",
    "    while i < len(lines)-1:\n",
    "        i+=1\n",
    "        if \"Passed-Metacommand type : MHA\" in lines[i]:\n",
    "            info_dic =dict()\n",
    "            mc_type = lines[i].rstrip().split(\"type :\")[-1]\n",
    "            gemm0_kernel_name = \"\"\n",
    "            gemm0_A_size = \"\"\n",
    "            gemm0_B_size = \"\"\n",
    "            gemm0_Output_size = \"\"\n",
    "\n",
    "            gemm1_kernel_name = \"\"\n",
    "            gemm1_A_size = \"\"\n",
    "            gemm1_B_size = \"\"\n",
    "            gemm1_Output_size = \"\"\n",
    "\n",
    "            for k in range(50):\n",
    "                if \"m_mhaGemm0Desc.ADesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm0_A_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"m_mhaGemm0Desc.BDesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm0_B_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"m_mhaGemm0Desc.OutputDesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm0_Output_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"MHA Gemm0 Shader Code\" in lines[i-k] or \"BLOB Gemm Kernel\" in lines[i-k]:\n",
    "                    if \"MHA Gemm0 Shader Code\" in lines[i-k]:\n",
    "                        gemm0_kernel_name =  lines[i-k].split('=')[1][:-1]\n",
    "                    else:\n",
    "                        gemm0_kernel_name = lines[i-k].split(\"BLOB Gemm Kernel\")[-1].strip().split(\".cpp\")[0]\n",
    "                    continue\n",
    "                \n",
    "                if \"m_mhaGemm1Desc.ADesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm1_A_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"m_mhaGemm1Desc.BDesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm1_B_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"m_mhaGemm1Desc.OutputDesc.Size\" in lines[i-k]:\n",
    "                    print(lines[i-k])\n",
    "                    gemm1_Output_size =  re.findall(r'[[](.*?)[]]', lines[i-k])[0]\n",
    "                    continue\n",
    "                if \"MHA Gemm1 Shader Code\" in lines[i-k] or \"BLOB Gemm Kernel\" in lines[i-k]:\n",
    "                    if \"MHA Gemm0 Shader Code\" in lines[i-k]:\n",
    "                        gemm1_kernel_name =  lines[i-k].split('=')[1][:-1]\n",
    "                    else:\n",
    "                        gemm1_kernel_name = lines[i-k].split(\"BLOB Gemm Kernel\")[-1].strip().split(\".cpp\")[0]\n",
    "                    continue\n",
    "            \n",
    "            writer.writerow([mc_type,gemm0_kernel_name, gemm0_A_size, gemm0_B_size,gemm0_Output_size,\n",
    "                                gemm1_kernel_name, gemm1_A_size, gemm1_B_size,gemm1_Output_size])\n",
    "\n",
    "    csvf.close()\n",
    "    print(\"{} generated\".format(csv_file))\n",
    "    df = pd.read_csv(csv_file)\n",
    "    s = pd.pivot_table(df, index=['gemm0_kernel_name', \"gemm1_kernel_name\"], aggfunc={\"gemm0_kernel_name\": \"count\", })\n",
    "    s.columns=['count']\n",
    "    ddi_conv_pivot_table = s.sort_values(by='count', ascending=0)\n",
    "    ddi_conv_pivot_table.to_csv(os.path.join(root_path, \"ddi_mha_pivot_table.csv\"))\n",
    "    print(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Intel\\igfx\\d3d12\"\n",
    "basename = \"python0.json\"\n",
    "count = 0\n",
    "\n",
    "df = pd.read_json(os.path.join(root_path,basename))\n",
    "#print(basename)\n",
    "str_info = basename+\"\t\"\n",
    "conv =df[df.keys()[0]]\n",
    "\n",
    "for key in conv.keys():\n",
    "    print(key, len(conv[key]))\n",
    "    #for i in range(len(conv[key])):\n",
    "\n",
    "\n",
    "#print(count) \n",
    "    #print(str_info,f32_count,f16_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "# root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "# basename = \"Adobe-Ae_FastMask_Query1024.log\"\n",
    "#create_ddi_json(root_path, basename)\n",
    "\n",
    "root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "files = glob.glob(root_path+\"\\\\*.log\")\n",
    "\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    #print(basename)\n",
    "    create_ddiPool_json(root_path, basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert json to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\DDIlog\\model_Sept\"\n",
    "files = glob.glob(root_path+\"\\\\*_conv.json\")\n",
    "\n",
    "csv_file = os.path.join(root_path,\"DDI_conv.csv\")\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer =csv.writer(csvf)\n",
    "line = [\"model_name\", \"kernel_name\",\"input_shape_n\",\"input_shape_c\",\"input_shape_h\",\"input_shape_w\", \"input_layout\", \"input_datatype\",\"input_flag\",\"input_padding\",\\\n",
    "        \"filter_shape_n\",\"filter_shape_c\",\"filter_shape_h\",\"filter_shape_w\", \"filter_layout\", \"filter_datatype\",\"filter_flag\", \"filter_stride_h\", \"filter_stride_w\", \"filter_stride_c\", \"filter_dilation_h\", \"filter_dilation_w\", \"filter_dilation_c\", \"filter_groupcount\",\n",
    "        \"output_shape_n\",\"output_shape_c\",\"output_shape_h\",\"output_shape_w\", \"output_layout\", \"output_datatype\",\"output_flag\", \"output_padding\", \n",
    "        \"bias\", \"direction\", \"activation\" ,\"exec_flag\"]\n",
    "writer.writerow(line)\n",
    "row_index = 0\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    df = pd.read_json(file)\n",
    "    conv =df[df.keys()[0]]\n",
    "    for key in conv.keys():\n",
    "        kernel_name = key\n",
    "        for i in range(len(conv[key])):\n",
    "            input_shape = conv[key][i][\"input\"][\"shape\"]\n",
    "            input_layout = conv[key][i][\"input\"][\"layout\"]\n",
    "            input_datatype = conv[key][i][\"input\"][\"datatype\"]\n",
    "            input_flag = conv[key][i][\"input\"][\"flag\"]\n",
    "            input_padding = [int(v) for v in  conv[key][i][\"input\"][\"padding\"]] \n",
    "\n",
    "            filter_shape = conv[key][i][\"filter\"][\"shape\"]\n",
    "            filter_layout = conv[key][i][\"filter\"][\"layout\"]\n",
    "            filter_datatype = conv[key][i][\"filter\"][\"datatype\"]\n",
    "            filter_flag = conv[key][i][\"filter\"][\"flag\"]\n",
    "            filter_stride = conv[key][i][\"filter\"][\"stirde\"]\n",
    "            filter_dilation = conv[key][i][\"filter\"][\"dilation\"]\n",
    "            filter_groupcount = conv[key][i][\"filter\"][\"groupcount\"]\n",
    "\n",
    "            output_shape = conv[key][i][\"output\"][\"shape\"]\n",
    "            output_layout = conv[key][i][\"output\"][\"layout\"]\n",
    "            output_datatype = conv[key][i][\"output\"][\"datatype\"]\n",
    "            output_flag = conv[key][i][\"output\"][\"flag\"]\n",
    "            output_padding = [int(v) for v in conv[key][i][\"output\"][\"padding\"]]\n",
    "\n",
    "            bias = conv[key][i][\"bias\"]\n",
    "            direction = conv[key][i][\"direction\"]\n",
    "            activation = conv[key][i][\"activation\"]\n",
    "            exec_flag = conv[key][i][\"exec_flag\"]\n",
    "            row_index +=1\n",
    "            if row_index % 2 ==0: #remove the repeated Conv info, may need to double check if there is some modification in application(ort or dml or driver)\n",
    "                continue\n",
    "            writer.writerow([basename, kernel_name, input_shape[0], input_shape[1], input_shape[2], input_shape[3], input_layout, input_datatype, input_flag,input_padding,\\\n",
    "                              filter_shape[0],filter_shape[1],filter_shape[2],filter_shape[3], filter_layout, filter_datatype, filter_flag, filter_stride[0], filter_stride[1],filter_stride[2],filter_dilation[0],filter_dilation[1],filter_dilation[2], filter_groupcount, \\\n",
    "                                output_shape[0],output_shape[1],output_shape[2],output_shape[3], output_layout, output_datatype, output_flag, output_padding,\n",
    "                                bias, direction, activation, exec_flag])\n",
    "    \n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GEMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "files = glob.glob(root_path+\"\\\\*_gemm.json\")\n",
    "\n",
    "csv_file = os.path.join(root_path,\"Adobe_GEMM.csv\")\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer =csv.writer(csvf)\n",
    "line = [\"model_name\", \"kernel_name\",\"inputA_shape_M\",\"inputA_shape_K\", \"inputA_datatype\",\"inputA_flag\",\"transA\",\\\n",
    "       \"inputB_shape_K\",\"inputB_shape_N\", \"inputB_datatype\",\"inputB_flag\",\"transB\",\\\n",
    "       \"output_shape_M\",\"output_shape_N\", \"output_datatype\",\"output_flag\", \"inputC_flag\", \"alpha\",\"beta\" ]\n",
    "writer.writerow(line)\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    df = pd.read_json(file)\n",
    "    if len(df.keys()) == 0:\n",
    "        continue\n",
    "    #print(basename)\n",
    "    gemm =df[df.keys()[0]]\n",
    "    for key in gemm.keys():\n",
    "        kernel_name = key\n",
    "        for i in range(len(gemm[key])):\n",
    "            inputA_shape = gemm[key][i][\"inputA\"][\"shape\"]\n",
    "            inputA_datatype = gemm[key][i][\"inputA\"][\"datatype\"]\n",
    "            inputA_flag = gemm[key][i][\"inputA\"][\"flag\"]\n",
    "            transA = gemm[key][i][\"inputA\"][\"transA\"]\n",
    "\n",
    "            inputB_shape = gemm[key][i][\"inputB\"][\"shape\"]\n",
    "            inputB_datatype = gemm[key][i][\"inputB\"][\"datatype\"]\n",
    "            inputB_flag = gemm[key][i][\"inputB\"][\"flag\"]\n",
    "            transB = gemm[key][i][\"inputB\"][\"transB\"]\n",
    "\n",
    "            output_shape = gemm[key][i][\"output\"][\"shape\"]\n",
    "            output_datatype = gemm[key][i][\"output\"][\"datatype\"]\n",
    "            output_flag = gemm[key][i][\"output\"][\"flag\"]\n",
    "\n",
    "            inputC_flag = gemm[key][i][\"inputC\"]\n",
    "            alpha = gemm[key][i][\"config\"][\"alpha\"]\n",
    "            beta = gemm[key][i][\"config\"][\"beta\"]\n",
    "\n",
    "            writer.writerow([basename, kernel_name, inputA_shape[0], inputA_shape[1], inputA_datatype, inputA_flag,transA,\\\n",
    "                              inputB_shape[0], inputB_shape[1], inputB_datatype, inputB_flag,transB, \\\n",
    "                                output_shape[0],output_shape[1],output_datatype, output_flag,\\\n",
    "                                 inputC_flag,alpha, beta])\n",
    "    \n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "files = glob.glob(root_path+\"\\\\*_pool.json\")\n",
    "\n",
    "csv_file = os.path.join(root_path,\"Adobe_Pooling.csv\")\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer =csv.writer(csvf)\n",
    "# the name is mapping to https://onnx.ai/onnx/operators/onnx__MaxPool.html#maxpool\n",
    "line = [\"model_name\", \"kernel_name\",\"pool_function\", \\\n",
    "       \"input_layout\", \"input_shape_n\",\"input_shape_c\",\"input_shape_h\",\"input_shape_w\", \"input_datatype\",\"input_flag\",\\\n",
    "       \"kernel_shape_h\",\"kernel_shape_w\", \"stride_h\", \"stride_w\",\"pads\",\\\n",
    "       \"output_layout\", \"output_shape_n\",\"output_shape_c\", \"output_shape_h\",\"output_shape_w\", \"output_datatype\",\"output_flag\" ]\n",
    "writer.writerow(line)\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    df = pd.read_json(file)\n",
    "    if len(df.keys()) == 0:\n",
    "        continue\n",
    "    #print(basename)\n",
    "    pool =df[df.keys()[0]]\n",
    "    for key in pool.keys():\n",
    "        kernel_name = key\n",
    "        for i in range(len(pool[key])):\n",
    "            pool_function = pool[key][i][\"pool_function\"]\n",
    "            input_layout = pool[key][i][\"input\"][\"layout\"]\n",
    "            input_shape = pool[key][i][\"input\"][\"shape\"]\n",
    "            input_datatype = pool[key][i][\"input\"][\"datatype\"]\n",
    "            input_flag = pool[key][i][\"input\"][\"flag\"]\n",
    "\n",
    "            filter_shape =  pool[key][i][\"filter\"][\"shape\"]\n",
    "            filter_stride =  pool[key][i][\"filter\"][\"stride\"]\n",
    "            filter_padding =  pool[key][i][\"filter\"][\"padding\"]\n",
    "\n",
    "            output_layout = pool[key][i][\"output\"][\"layout\"]\n",
    "            output_shape = pool[key][i][\"output\"][\"shape\"]\n",
    "            output_datatype = pool[key][i][\"output\"][\"datatype\"]\n",
    "            output_flag = pool[key][i][\"output\"][\"flag\"]\n",
    "\n",
    "            writer.writerow([basename, kernel_name, pool_function,\n",
    "                             input_layout, input_shape[0], input_shape[1], input_shape[2], input_shape[3],input_datatype, input_flag, \\\n",
    "                                filter_shape[0],filter_shape[1], filter_stride[0],filter_stride[1], filter_padding,\\\n",
    "                                output_layout, output_shape[0], output_shape[1], output_shape[2], output_shape[3],output_datatype, output_flag ])\n",
    "    \n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_up_next_multiple(n,m):\n",
    "    return ((n + m - 1) / m) * m\n",
    "\n",
    "values = \"-0.125488;0.279785;-0.343994;-0.166260;-0.479492;0.438477;-0.316650;-0.492920;0.111877;0.473877;0.285156;0.483154;0.107544;-0.486816;0.308350;-0.269043;-0.377930;-0.326660;0.162476;-0.291992;0.469482;-0.104858;0.421875;0.020828;-0.111328;0.086731;0.042694;-0.334717;0.272217;-0.485840;0.229004;0.426270;0.363037;-0.404541;-0.174805;-0.225220;-0.380371;0.221680;-0.006203;-0.389160;-0.468506;0.063293;-0.250732;-0.296875;-0.210205;0.124329;0.371582;-0.083496;0.307373;0.406738;-0.072876;-0.195190;-0.082581;-0.230591;-0.176758;-0.096191;0.462402;0.212280;-0.463135;-0.088989;0.408203;0.030930;-0.258057;-0.130371;-0.132202;-0.101196;0.335205;0.195801;0.177612;0.309570;-0.325684;0.017746;-0.158936;0.041443;0.160034;0.016632;-0.406982;0.382568;-0.150757;0.356445;0.296631;0.098633;-0.400146;0.101135;-0.443604;-0.287598;0.117493;-0.068054;-0.100159;-0.133667;0.118408;0.092407;0.180298;0.448975;-0.114563;0.184204;0.109985;0.409424;0.255371;0.046722;0.342285;0.394775;-0.173462;-0.454834;0.247314;-0.143188;-0.223999;-0.425537;-0.105103;0.315430;0.290283;-0.141479;0.350098;-0.436523;0.165894;0.387207;0.471680;0.061279;-0.459473;-0.474609;0.395752;0.008568;0.104431;-0.271240;0.194824;0.308105;-0.043457;0.392578;-0.377930;-0.389893;-0.499512;-0.493164;-0.015167;-0.162354;-0.281250;-0.136353;-0.253174;-0.199097;-0.233276;-0.448486;0.134399;-0.010544;0.092712;-0.262451;-0.029694;0.035767;-0.349365;-0.459229;-0.279785;-0.273438;0.440430;0.436768;0.235229;0.377441;-0.324951;0.029648;0.465332;0.133057;0.200317;0.387207;0.450684;0.096863;-0.441895;-0.357178;0.469971;-0.499268;-0.195801;-0.477051;-0.360596;-0.267334;-0.300293;-0.033234;-0.329590;0.442139;-0.195435;-0.259033;-0.004822;-0.108948;-0.188232;0.067688;0.275146;0.426758;-0.411621;0.461182;-0.228638;0.465332;-0.359131;-0.484375;-0.301270;-0.301270;0.271240;0.151123;0.123291;-0.129150;0.229614;0.061249;0.213257;-0.263916;0.022736;-0.060669;0.136353;0.195557;-0.089600;0.442871;-0.338867;-0.204346;0.303711;0.383301;0.395996;-0.227905;0.318115;-0.335449;-0.277832;-0.255859;0.018784;-0.435059;-0.248169;-0.351807;0.109558;-0.467041;-0.260498;-0.052216;0.172119;-0.257812;0.132324;0.316406;-0.179199;0.358398;-0.483398;-0.151367;0.190918;0.337646;-0.386475;0.195801;0.317139;-0.239136;0.397217;-0.311279;0.225952;-0.095520;-0.316650;-0.343994;-0.040741;0.208130;0.222046;-0.318115;0.111633;-0.208740;-0.453369;-0.043915;-0.117554;-0.453613;-0.049500;0.465576;-0.484131;-0.059845;0.333252;-0.241211;-0.074829;-0.315186;-0.050232;0.097900;0.070435;-0.174683;0.039703;-0.219116;-0.203735;0.486816;-0.206543;0.206909;0.105957;-0.384033;-0.050537;-0.188965;0.091309;-0.027786;0.348877;0.270996;0.210693;-0.392090;-0.024628;0.407471;0.039856;-0.423096;0.380371;0.133423;-0.281494;0.039337;-0.143677;-0.271973;-0.147461;0.010750;0.192383;0.442871;0.058105;0.471680;0.196289;-0.215210;0.476562;-0.221313;0.180664;0.485596;-0.419189;0.228271;0.483398;-0.409668;0.008202;0.090881;0.211182;0.145142;-0.102417;-0.362549;-0.291016;-0.242065;0.482178;-0.258057;0.058289;-0.161011;0.346680;0.279785;0.231934;-0.054169;0.366211;0.150879;0.332520;0.492188;0.024750;0.024780;-0.207886;-0.409424;0.014236;0.359863;-0.435059;0.063293;-0.402344;0.183228;-0.465576;-0.317871;0.020065;-0.468750;0.439453;0.227295;-0.303955;0.344482;0.328857;0.107056;0.302246;-0.076599;-0.494385;0.211304;-0.426025;0.415039;-0.169067;0.168823;0.137573;-0.117065;0.260742;-0.243896;-0.072449;-0.298340;-0.185669;-0.360596;0.255615;0.098877;0.429688;-0.394531;-0.313477;-0.175659;-0.182007;0.147705;0.360840;0.034088;-0.380127;-0.331787;0.203003;-0.246094;-0.002752;0.497803;0.002680;-0.154907;-0.355225;0.052887;0.261719;0.303223;0.133545;0.298340;-0.313477;-0.174072;0.012093;-0.403809;-0.113281;0.175659;0.424805;-0.271484;0.055206;0.496338;0.400391;-0.221069;0.397217;0.387695;0.279785;-0.343994;-0.166260;-0.479492;0.438477;-0.316650;-0.492920;0.111877;0.473877;0.285156;0.483154;0.107544;-0.486816;0.308350;-0.269043;-0.377930;-0.326660;0.162476;-0.291992;0.469482;-0.104858;0.421875;0.020828;-0.111328;0.086731;0.042694;-0.334717;0.272217;-0.485840;0.229004;0.426270;0.363037;-0.404541;-0.174805;-0.225220;-0.380371;0.221680;-0.006203;-0.389160;-0.468506;0.063293;-0.250732;-0.296875;-0.210205;0.124329;0.371582;-0.083496;0.307373;0.406738;-0.072876;-0.195190;-0.082581;-0.230591;-0.176758;-0.096191;0.462402;0.212280;-0.463135;-0.088989;0.408203;0.030930;-0.258057;-0.130371;-0.132202;-0.101196;0.335205;0.195801;0.177612;0.309570;-0.325684;0.017746;-0.158936;0.041443;0.160034;0.016632;-0.406982;0.382568;-0.150757;0.356445;0.142090;0.098633;-0.400146;0.101135;-0.443604;-0.287598;0.117493;-0.068054;-0.100159;-0.133667;0.118408;0.092407;0.180298;0.448975;-0.114563;0.184204;0.109985;0.409424;0.255371;0.046722;0.342285;0.394775;-0.173462;-0.454834;0.247314;-0.143188;-0.223999;-0.425537;-0.105103;0.315430;0.290283;-0.141479;0.350098;-0.436523;0.165894;0.387207;0.471680;0.061279;-0.459473;-0.474609;0.395752;0.008568;0.104431;-0.271240;0.194824;0.308105;-0.043457;0.392578;-0.377930;-0.389893;-0.499512;-0.493164;-0.015167;-0.162354;-0.281250;-0.136353;-0.253174;-0.199097;-0.233276;-0.448486;0.134399;-0.010544;0.092712;-0.262451;-0.029694;0.035767;-0.349365;-0.459229;-0.279785;-0.273438;0.440430;0.436768;0.235229;0.377441;-0.324951;0.029648;0.465332;0.133057;0.200317;0.387207;0.350830;0.096863;-0.441895;-0.357178;0.469971;-0.499268;-0.195801;-0.477051;-0.360596;-0.267334;-0.300293;-0.033234;-0.329590;0.442139;-0.195435;-0.259033;-0.004822;-0.108948;-0.188232;0.067688;0.275146;0.426758;-0.411621;0.461182;-0.228638;0.465332;-0.359131;-0.484375;-0.301270;-0.301270;0.271240;0.151123;0.123291;-0.129150;0.229614;0.061249;0.213257;-0.263916;0.022736;-0.060669;0.136353;0.195557;-0.089600;0.442871;-0.338867;-0.204346;0.303711;0.383301;0.395996;-0.227905;0.318115;-0.335449;-0.277832;-0.255859;0.018784;-0.435059;-0.248169;-0.351807;0.109558;-0.467041;-0.260498;-0.052216;0.172119;-0.257812;0.132324;0.316406;-0.179199;0.358398;-0.483398;-0.151367;0.190918;0.337646;-0.386475;0.195801;0.317139;-0.239136;0.397217;-0.311279;0.225952;-0.095520;-0.415771\"\n",
    "values_list = [float(i) for i in values.split(\";\")]\n",
    "print(len(values_list[::5]))\n",
    "print(values_list[::5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='section1'>Extract DML PIX info</a>\n",
    "* Before using the script below, please cget the PIX log first. You can get step-by-step guide from this recording [PIX GPU capture training-20220314_155822-Meeting Recording.mp4](https://intel.sharepoint.com/sites/applied_ai/_layouts/15/stream.aspx?id=%2Fsites%2Fapplied%5Fai%2FShared%20Documents%2FClient%20AI%2FPIX%20GPU%20capture%20training%2D20220314%5F155822%2DMeeting%20Recording%2Emp4&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\"\n",
    "log_file = \"PIX_log.txt\"\n",
    "file = os.path.join(rootpath, log_file)\n",
    "file = file.strip(\".txt\")\n",
    "rawdata = pd.read_csv(f\"{file}.txt\",delimiter = '\\t')\n",
    "\n",
    "signal_count = 0\n",
    "first_iteration_start = 0\n",
    "second_iteration_start = 0  \n",
    "## Option 1:\n",
    "#  use \"DML_EXECUTION_PLAN\" to find where is the start of first_iteration and second iteration\n",
    "#  value = line number - 1\n",
    "#  \n",
    "## Option 2:\n",
    "#  use the algorithm below to find first iteration and second iteration\n",
    "#  but most of the time it does not work\n",
    "'''\n",
    "for index, line in rawdata.iterrows():\n",
    "    if signal_count == 5:\n",
    "        first_iteration = index\n",
    "    if signal_count == 10:\n",
    "        second_iteration = index\n",
    "        break\n",
    "    if \"Signal\" in line :\n",
    "        signal_count +=0\n",
    "'''\n",
    "## Option 3: [todo] use the whole information to decide\n",
    "\n",
    "first_iteration_start = 180\n",
    "second_iteration_start = 960\n",
    "prevline = \"\"\n",
    "ex_operator_list=[]\n",
    "ex_time = []\n",
    "\n",
    "dispatch_operator_list=[]\n",
    "dispatch_time=[]\n",
    "\n",
    "pre_checkDispatch =False\n",
    "idx = -1\n",
    "while idx < len(rawdata)-2: \n",
    "    idx+=1\n",
    "    if idx < first_iteration_start:\n",
    "        continue\n",
    "    if idx > second_iteration_start:\n",
    "        break\n",
    "    line = rawdata.iloc[idx]\n",
    "    if \"ExecuteMetaCommand\" in line[2]:      \n",
    "        ex_operator_list.append((prevline[2].strip()))\n",
    "        ex_time.append(int(prevline[4]))\n",
    "        # temp.append(line)\n",
    "    if \"Dispatch\" in line[2]:\n",
    "        if pre_checkDispatch:\n",
    "            continue\n",
    "        dispatch_operator_list.append((prevline[2].strip()))\n",
    "        dispatch_time.append(int(prevline[4]))\n",
    "        pre_checkDispatch = True \n",
    "    else:\n",
    "        prevline = line\n",
    "        pre_checkDispatch = False\n",
    "\n",
    "sumup= (sum(dispatch_time) + sum(ex_time))/1000000\n",
    "print(\"total latency per iteration: {} ms \\n \\\n",
    "      Tip: if the data is too different from ort_perf_test.exe,\\n \\\n",
    "      please double check the first/second iteration number\".format(round(sumup,2)))\n",
    "\n",
    "\n",
    "csv_file = f\"{file}_test.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"execute type\",\"layer type\",\"layer name\",\"time\"]\n",
    "writer.writerow(line)\n",
    "\n",
    "for op,time in zip(ex_operator_list,ex_time):\n",
    "    layer_info = op.split(\",\")[-1]\n",
    "    mark_idx = layer_info.index(\"(\")\n",
    "    layer_type = layer_info[0:mark_idx-1]\n",
    "    layer_name = layer_info[mark_idx+1:-1]\n",
    "    writer.writerow([\"ExecuteMetaCommand\",layer_type, layer_name,round(float(time)/1000000,2)])\n",
    "\n",
    "for op,time in zip(dispatch_operator_list,dispatch_time):\n",
    "    layer_info = op.split(\",\")[-1]\n",
    "    if \"(\" in layer_info:\n",
    "        mark_idx = layer_info.index(\"(\")\n",
    "        layer_type = layer_info[0:mark_idx-1]\n",
    "        layer_name = layer_info[mark_idx+1:-1]\n",
    "    else:# some op may not have infomation in (), e.g.DML_OPERATOR_ACTIVATION_GELU\n",
    "        mark_idx = 0\n",
    "        layer_type = layer_info[0:]\n",
    "        layer_name = \"unknown\"\n",
    "    writer.writerow([\"Dispatch\",layer_type, layer_name,round(float(time)/1000000,2)])\n",
    "\n",
    "\n",
    "csvf.close()\n",
    "print(\"{} generated\".format(csv_file))\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(os.path.join(rootpath, csv_file))\n",
    "filtered_df1 = df1[(df1['layer type'] == 'DML_OPERATOR_CONVOLUTION') & (df1['execute type'] == 'ExecuteMetaCommand')]\n",
    "filtered_df1.to_csv(os.path.join(rootpath, 'temp.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different pix log\n",
    "import pandas as pd\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\\helpWindow\\for_prithv\"\n",
    "log1 = \"v0_pixlog_1_test.csv\"\n",
    "log2 = \"v1_Emptyreorder_pixlog_test.csv\"\n",
    "log3 = \"origin_pixlog_test.csv\"\n",
    "df1 = pd.read_csv(os.path.join(rootpath, log1))\n",
    "df2 = pd.read_csv(os.path.join(rootpath, log2))\n",
    "df3 = pd.read_csv(os.path.join(rootpath, log3))\n",
    "\n",
    "filtered_df1 = df1[(df1['layer type'] == 'DML_OPERATOR_GEMM') & (df1['execute type'] == 'ExecuteMetaCommand')]\n",
    "filtered_df2 = df2[(df2['layer type'] == 'DML_OPERATOR_GEMM') & (df2['execute type'] == 'ExecuteMetaCommand')]\n",
    "filtered_df3 = df3[(df3['layer type'] == 'DML_OPERATOR_GEMM') & (df3['execute type'] == 'ExecuteMetaCommand')]\n",
    "\n",
    "merged_df = filtered_df1.merge(filtered_df2, on='layer name').merge(filtered_df3, on='layer name')  #df1  x, df2 y\n",
    "merged_df.to_csv(os.path.join(rootpath, 'compare_gemm.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section4'>Extract onnxruntime profiling data </a>\n",
    "* take the unet model as an example, `onnxruntime_perf_test.exe -m times -r 1 -f unet_sample_batch:2 -f unet_sample_channels:4 -f unet_sample_width:64 -f unet_sample_height:64 -f unet_hidden_batch:2 -f unet_hidden_sequence:77 -I -u temp.onnx -p ort_profiling.json -e dml model_path`\n",
    "  - `-u` is used to avoid only one DML fused node is dummped\n",
    "  - `-p` is used to generate information like layer name, input and output shape.\n",
    "* Convert ort profling json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# index value in profile is same with allocator_planner.cc\n",
    "# index value in tensor allocation is different, where is it?\n",
    "import csv\n",
    "import json\n",
    "time_file = r'C:\\Users\\GAME\\Documents\\Project\\INT8\\mobilenetv2-12-int8\\mobilenet_dml.json_2023-12-12_17-04-08.json'\n",
    "csv_file = time_file.replace(\"json\", 'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    " # only has three inputs and  one output as defualt, may need to change as needed\n",
    "line = [\"name\",\"op_type\", \"input1_type\", \"input1_shape\", \"input2_type\", \"input2_shape\",\"input3_type\", \"input3_shape\",\"output_type\",\"output_shape\",\"duration\"]\n",
    "writer.writerow(line)\n",
    "count = 0\n",
    "node_count =0\n",
    "total_duration = 0\n",
    "count_time={}\n",
    "count_name = {}\n",
    "other_info={}\n",
    "with open(time_file, 'r') as f:\n",
    "    # load the contents of the file into a dictionary\n",
    "    data = json.load(f)\n",
    "    for i in range(len(data)):\n",
    "        # if \"/conv_in/Conv_fence_before\" in data[i][\"name\"]:\n",
    "        #     count += 1\n",
    "        # if count == 2:\n",
    "        #     break\n",
    "        if \"kernel\" in data[i][\"name\"]: \n",
    "            output_name = data[i][\"name\"].split(\"_kernel\")[0]\n",
    "            if output_name not in count_time:\n",
    "                count_time[output_name] = int(data[i][\"dur\"])\n",
    "                count_name[output_name] = 1\n",
    "\n",
    "                \n",
    "                op_type = data[i]['args']['op_name']\n",
    "                \n",
    "                output_size = data[i]['args']['output_size']\n",
    "                input_type = [\"none\"]*3\n",
    "                input_shape = [[]] * 3\n",
    "                output_type = None\n",
    "                output_shape = None\n",
    "                for j, p in enumerate(data[i]['args']['input_type_shape']):\n",
    "                    if j == 3:\n",
    "                        break\n",
    "                    for key, value in p.items():\n",
    "                        input_type[j]= key\n",
    "                        input_shape[j]= value\n",
    "                for j, p in enumerate(data[i]['args']['output_type_shape']):\n",
    "                    for key, value in p.items():\n",
    "                        output_type = key\n",
    "                        output_shape = value\n",
    "                        break\n",
    "                    break\n",
    "\n",
    "                other_info[output_name] = [output_name, op_type, input_type[0],input_shape[0],\\\n",
    "                                            input_type[1],input_shape[1],\n",
    "                                                input_type[2], input_shape[2],\n",
    "                                                output_type,output_shape]\n",
    "            else:\n",
    "                count_time[output_name] += int(data[i][\"dur\"])\n",
    "                count_name[output_name] += 1\n",
    "\n",
    "\n",
    "for key, value in count_time.items():\n",
    "    duration = round(value / count_name[key],2)\n",
    "    total_duration+=duration\n",
    "    kernel_info = other_info[key]\n",
    "    \n",
    "    kernel_info.append(duration)\n",
    "    writer.writerow(kernel_info)\n",
    "    node_count+=1\n",
    "csvf.close()\n",
    "print(\"total duration:\",round(total_duration/1000,2))\n",
    "print(\"Done to generate file {}\". format(csv_file))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <a id='section5'> DML&OV Layer By Layer profiling </a> \n",
    "* Step1: collect CSV1:  DML info from [Extract PIX log](#section1) which has <font color='orange'>the layer name</font> and DML performance data for each layer\n",
    "* Step2: collect CSV2:  DML DDI info from [Extract DDI log](#section2) which has the <font color='red'>input/output shape/datatype/layout</font> for each layer\n",
    "* Step3: collect CSV3:  OV info from [Extract OV performance data](#section3) which has <font color='orange'>the layer name</font>  and OV performance data for each layer\n",
    "* Step4: collect CSV4:  model info from [onnxruntime profiling data](#section4) which has <font color='orange'>the layer name</font>  and the <font color='red'>input/output shape/datatype</font> for each layer  \n",
    "* Step5: Once these four CSVs are collected, we can connect them based on the unique info. For example,  <font color='orange'>layer name</font>  in yellow;  <font color='red'>input/output shape/datatype</font> in red. Some info may not be exactly same, need to manually check\n",
    "> Here is a reference for automatically combining ORT info with DML and OV performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Take layer name as a unique key\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\2023Q4_Profiling_sd1.5\"\n",
    "perf_file = \"new_unet_onnx_info.csv\" # this is a file has [Layer name, DML perf, OV perf]\n",
    "node_csv = r\"new_issue\\profiling.csv_2023-10-12_11-33-55.csv\"\n",
    "new_file = open(os.path.join(rootpath,perf_file+\".csv\"),\"w\")\n",
    "node_file =  open(os.path.join(rootpath, node_csv))\n",
    "used_line_index = []\n",
    "new_file.write(\"DML, OV, input1_layout,Layer name, op_type,  input1_type, input1_shape, input2_type, input2_shape,input3_type, input3_shape,output_type,output_shape\\n\")\n",
    "for i, nline in enumerate(node_file.readlines()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    node_info = nline.split(\",\")\n",
    "    layer_name = node_info[0]\n",
    "    ddi_file = open(os.path.join(rootpath, perf_file))\n",
    "    for j, dline in enumerate(ddi_file.readlines()):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        info_list = dline.rstrip().split(\",\")      \n",
    "        perf_layer_name = info_list[0]\n",
    "        input_layout = \"NONE\"\n",
    "        if \"Conv\" in node_info[1]:\n",
    "            # check layout\n",
    "            # this layout is not aligned with DDI LOG\n",
    "            # Because DML may have optimization like NCHW <=> HLSL Layout conversion + NHWC\n",
    "            pattern = re.compile(r'\\[([^\\]]+)\\]')\n",
    "            matches = pattern.findall(nline)\n",
    "            input_shape = matches[0].split(\",\")\n",
    "            if input_shape[2] == input_shape[3]:\n",
    "                input_layout = \"NCHW\"\n",
    "            else:\n",
    "                input_layout = \"NHWC\"\n",
    "        if layer_name == perf_layer_name \\\n",
    "            and j not in used_line_index:\n",
    "            used_line_index.append(j)       \n",
    "            newline = \",\".join(info_list[1:3])+\",\"+input_layout+\",\"+nline\n",
    "            new_file.write(newline)\n",
    "            #break\n",
    "    ddi_file.close()\n",
    "node_file.close()\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution: link to DDI log\n",
    "# Take \"input_shape, filter_shape, output_shaper\" as a unique key\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\2023Q4_Profiling_sd1.5\"\n",
    "ddi_csv = r\"DDI_conv.csv\"\n",
    "ort_perf_csv = \"unet_onnx_info.csv.csv\"\n",
    "ort_perf_file =  open(os.path.join(rootpath, ort_perf_csv))\n",
    "new_file = open(os.path.join(rootpath,ort_perf_csv+\"_ddi.csv\"),\"w\")\n",
    "\n",
    "new_file.write(\"Status, DML,OV, input1_layout,Layer name,model_name,kernel_name,input_shape_n,input_shape_c,input_shape_h,input_shape_w,input_layout,input_datatype,input_flag,input_padding,filter_shape_n,filter_shape_c,filter_shape_h,filter_shape_w,filter_layout,filter_datatype,filter_flag,filter_stride_h,filter_stride_w,filter_stride_c,filter_dilation_h,filter_dilation_w,filter_dilation_c,filter_groupcount,output_shape_n,output_shape_c,output_shape_h,output_shape_w,output_layout,output_datatype,output_flag,output_padding,bias,direction,activation,exec_flag \\n\")\n",
    "used_line_index = []\n",
    "matched_count = 0\n",
    "rest_count = 0\n",
    "for i, nline in enumerate(ort_perf_file.readlines()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    pattern = re.compile(r'\\[([^\\]]+)\\]')\n",
    "    matches = pattern.findall(nline)\n",
    "    ort_input_shape = matches[0]\n",
    "    ort_filter_shape = matches[1]\n",
    "    ort_output_shape = matches[-1]\n",
    "    op_type = nline.split(\",\")[4]\n",
    "\n",
    "    nline_new = \",\".join(nline.split(\",\")[:4])\n",
    "    matched_list = []\n",
    "    temp_used_line_index= []\n",
    "    if  op_type == \"Conv\" :\n",
    "        shape = node_info[-1].rstrip()\n",
    "        #print(shape)\n",
    "        ddi_file = open(os.path.join(rootpath, ddi_csv))\n",
    "        for j, dline in enumerate(ddi_file.readlines()):\n",
    "            if j == 0:\n",
    "                continue           \n",
    "            info_list = dline.rstrip().split(\",\")\n",
    "            input_shape  = \", \".join(info_list[2:6])\n",
    "            filter_shape = \", \".join(info_list[15:19])\n",
    "            output_shape = \", \".join(info_list[29:33])\n",
    "            #print(input_shape, ort_input_shape)\n",
    "            #print(filter_shape, ort_filter_shape)\n",
    "            #print(output_shape, ort_output_shape)\n",
    "            if ort_input_shape == input_shape  \\\n",
    "                    and ort_filter_shape == filter_shape \\\n",
    "                    and ort_output_shape == output_shape \\\n",
    "                     and j not in used_line_index:\n",
    "                temp_used_line_index.append(j)             \n",
    "                matched_list.append(dline)\n",
    "        no_repeat_match = list(set(matched_list))\n",
    "        if len(no_repeat_match) == 1: # if ddi log is same, then assign one directly\n",
    "            matched_count += 1\n",
    "            used_line_index.append(temp_used_line_index[0])\n",
    "            newline = \"Valid,\"+nline_new + \",\"+no_repeat_match[0]\n",
    "            new_file.write(newline)\n",
    "        else:\n",
    "            #print(\"{} cannot find\".format(nline_new))\n",
    "            rest_count +=1\n",
    "            for item in no_repeat_match:\n",
    "                newline = \"Uncertian,\"+nline_new + \",\"+ item\n",
    "                new_file.write(newline)\n",
    "\n",
    "new_file.close()\n",
    "print(\"Summary:\\n \\\n",
    "      {}/{} find a unique record of DDI LOG \\n \\\n",
    "      rest {} Convs find multiple records of DDI LOGS \". format(matched_count, matched_count+rest_count, rest_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option2(Depreciate): \n",
    "* hack into netron to dump model information: layer name, weight shape etc.\n",
    "* However, information about input shape cannot be captured because it is calculated in runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\2023Q4_Profiling_sd1.5\"\n",
    "ddi_csv = r\"new_issue\\new_DDI_conv.csv\"\n",
    "node_csv = \"new_unet_onnx_info.csv\"\n",
    "new_file = open(os.path.join(rootpath,node_csv+\".csv\"),\"w\")\n",
    "node_file =  open(os.path.join(rootpath, node_csv))\n",
    "\n",
    "used_line_index = []\n",
    "new_file.write(\"Layer name,DML,OV, model_name,kernel_name,input_shape_n,input_shape_c,input_shape_h,input_shape_w,input_layout,input_datatype,input_flag,input_padding,filter_shape_n,filter_shape_c,filter_shape_h,filter_shape_w,filter_layout,filter_datatype,filter_flag,filter_stride_h,filter_stride_w,filter_stride_c,filter_dilation_h,filter_dilation_w,filter_dilation_c,filter_groupcount,output_shape_n,output_shape_c,output_shape_h,output_shape_w,output_layout,output_datatype,output_flag,output_padding,bias,direction,activation,exec_flag\\n\")\n",
    "for i, nline in enumerate(node_file.readlines()):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    node_info = nline.split(\";\")\n",
    "    layer_name = node_info[0]\n",
    "\n",
    "    shape = node_info[-1].rstrip().split(\",\")\n",
    "    if len(shape) == 4 :\n",
    "        shape = node_info[-1].rstrip()\n",
    "        #print(shape)\n",
    "        ddi_file = open(os.path.join(rootpath, ddi_csv))\n",
    "        for j, dline in enumerate(ddi_file.readlines()):\n",
    "            if j == 0:\n",
    "                continue\n",
    "            \n",
    "            info_list = dline.rstrip().split(\",\")      \n",
    "            filter_shape = \"[\"+\",\".join(info_list[15:19])+\"]\"\n",
    "            if shape == filter_shape \\\n",
    "                  and j not in used_line_index:\n",
    "                # if (\"proj_in\" in layer_name and info_list[6] == \"NHWC\") \\\n",
    "                #     or (\"proj_out\" in layer_name and info_list[6] == \"NCHW\"):            \n",
    "                #     continue\n",
    "                used_line_index.append(j)\n",
    "                \n",
    "                newline = \",\".join(node_info[0:3]) + \",\"+dline\n",
    "                new_file.write(newline)\n",
    "                break\n",
    "\n",
    "new_file.close()\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\2023Q4_Profiling_sd1.5\"\n",
    "log_file = \"unet_PIX.txt\"\n",
    "file = os.path.join(rootpath, log_file)\n",
    "file = file.strip(\".txt\")\n",
    "\n",
    "csv_file = f\"{file}_test.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "df = pd.DataFrame(data)\n",
    "pivot_table = df.pivot_table(values='time', index=['layer type','execute type'], aggfunc='sum')\n",
    "print(pivot_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract info from benchmark.bat log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DML backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"dml_log_release.txt\"\n",
    "csv_file = \"dml_perf_release.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"model name\",\"not save model\",\"save model\"]\n",
    "writer.writerow(line)\n",
    "with open(log_file,\"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    idx = 0\n",
    "    while idx < len(all_lines):\n",
    "        line = all_lines[idx]\n",
    "        if \"model_rename\" in line:\n",
    "            model_name = line.rstrip().split(\"model_rename\\\\\")[-1]\n",
    "            if \"Average inference\" not in all_lines[idx+6]:\n",
    "                perf_save = all_lines[idx+10].rstrip().split(\"time cost:\")[-1]  \n",
    "                writer.writerow([model_name, \"erro\", perf_save])\n",
    "                idx = idx + 25\n",
    "                print(idx)\n",
    "            else:\n",
    "                perf_nosave = all_lines[idx+6].rstrip().split(\"time cost:\")[-1]   \n",
    "                perf_save = all_lines[idx+26].rstrip().split(\"time cost:\")[-1]\n",
    "                writer.writerow( [model_name , perf_nosave, perf_save])\n",
    "                idx = idx+41\n",
    "csvf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"ov_log.txt\"\n",
    "csv_file = \"ov_perf.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"model name\",\"FP16\",\"FP32\"]\n",
    "writer.writerow(line)\n",
    "with open(log_file,\"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    idx = 0\n",
    "    while idx < len(all_lines):\n",
    "        line = all_lines[idx]\n",
    "        if \"model_rename\" in line:\n",
    "            model_name = line.rstrip().split(\"model_rename\\\\\")[-1]\n",
    "            print(model_name)\n",
    "            if \"time cost\" not in all_lines[idx+6]:\n",
    "                perf_save = all_lines[idx+7].rstrip().split(\"time cost:\")[-1]  \n",
    "                writer.writerow([model_name, \"erro\", perf_save])\n",
    "                idx = idx + 22\n",
    "            else:\n",
    "                perf_nosave = all_lines[idx+6].rstrip().split(\"time cost:\")[-1]   \n",
    "                perf_save = all_lines[idx+26].rstrip().split(\"time cost:\")[-1]\n",
    "                writer.writerow( [model_name , perf_nosave, perf_save])\n",
    "                idx = idx+41\n",
    "csvf.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='section3'> Extract OpenVINO performance data</a>\n",
    "## Option 1 if you have ort_perf_test.exe debug version:\n",
    "1. set `ORT_OPENVINO_ENABLE_DEBUG=1`, and run `ort_perf_test.exe  -m times -r 1 -I -e openvino -i \"device_type|GPU_FP32 cache_dir|ov_cache\" onnx_model_path`\n",
    "2. some performance logs will be appeared in the command window, here is an example below. You need to manually save it and process into csv file [The script below is some reference but it is not very good]\n",
    "```\n",
    "convolution10/WithoutBiases   EXECUTED       layerType: Convolution        realTime: 374       cpu: 0               execType: jit:ir__f16\n",
    "convolution10                 OPTIMIZED_OUT  layerType: Add                realTime: 0         cpu: 0               execType: undef\n",
    "activation10                  OPTIMIZED_OUT  layerType: Relu               realTime: 0         cpu: 0               execType: undef\n",
    "convolution11/WithoutBiases   EXECUTED       layerType: Convolution        realTime: 330       cpu: 0               execType: jit:ir__f16\n",
    "convolution11                 OPTIMIZED_OUT  layerType: Add                realTime: 0         cpu: 0               execType: undef\n",
    "activation11                  OPTIMIZED_OUT  layerType: Relu               realTime: 0         cpu: 0               execType: undef\n",
    "convolution12/WithoutBiases   EXECUTED       layerType: Convolution        realTime: 294       cpu: 0               execType: jit:ir__f16\n",
    "convolution12                 OPTIMIZED_OUT  layerType: Add                realTime: 0         cpu: 0               execType: undef\n",
    "activation12                  OPTIMIZED_OUT  layerType: Relu               realTime: 0         cpu: 0               execType: undef\n",
    "```\n",
    "3. The parsed onnx model will also be saved as `OpenVINOExecutionProvider_OpenVINO-EP-subgraph_1_0.onnx`, you can use this model to run `openvino benchmark_app` if the original model cannot be parsed by openvino\n",
    "\n",
    "## Option 2 (Recommend)\n",
    "1. set python environment: pip install openvino_dev\n",
    "2. this package contains a tool called benchmark_app\n",
    "3. run benchmark_app with the command line\n",
    "`benchmark_app -m your_onnx_model_path -d GPU -nireq 1 -niter 10 --report_type detailed_counters --report_folder perf\\`\n",
    "4. after running this, a performance csv file will be generated under `perf\\` folder\n",
    "\n",
    "\n",
    "> I think the performance result in Option2 is validated for profiling ORT_OV performance because  I have checked the pipeline in onnxruntime ov. From my understanding, ort only did some work about parsing onnx model and rest of the works are all handled by OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_file = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\Adobe-Ps_SuperZoom_V316.ov.txt\"\n",
    "csv_file = perf_file.replace(\"txt\",'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"layer_type\",\"layer name\",\"gpu(ms)\",\"cpu(ms)\"]\n",
    "writer.writerow(line)\n",
    "total_time = 0\n",
    "with open(perf_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line_items = line.rstrip().split(\" \")\n",
    "        info = [i for i in line_items if i != '']\n",
    "        #print(info)\n",
    "        layer_type = info[3]\n",
    "        layer_name = info[0]\n",
    "        gpu_time = int(info[5])/1000\n",
    "        cpu_time = int(info[7])/1000\n",
    "        total_time+=gpu_time\n",
    "        if gpu_time == 0.0:\n",
    "            continue\n",
    "        writer.writerow([layer_type,layer_name, gpu_time,cpu_time])\n",
    "print(\"total time: {:2f}\".format(total_time)) \n",
    "csvf.close()     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract info from ORT profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORT time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_file = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_time_all.json'\n",
    "csv_file = time_file.replace(\"json\", 'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"start\",\"duration\",\"name\",\"op_name\", \"provider\", \"role\", \"data_type\"]\n",
    "writer.writerow(line)\n",
    "with open(time_file, 'r') as f:\n",
    "    # load the contents of the file into a dictionary\n",
    "    data = json.load(f)\n",
    "    for i in range(len(data)):\n",
    "        temp = \"\"\n",
    "        if \"op_name\" not in data[i][\"args\"]:\n",
    "            continue\n",
    "        if \"provider\" not in data[i][\"args\"]:\n",
    "            temp = \"none\"\n",
    "            data_type = \"none\"\n",
    "            line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"none\", data_type]\n",
    "            writer.writerow(line)\n",
    "        else:\n",
    "            temp = data[i][\"args\"][\"provider\"]\n",
    "            inputs = data[i][\"args\"][\"input_type_shape\"]\n",
    "            outputs = data[i][\"args\"][\"output_type_shape\"]\n",
    "            for input in inputs:\n",
    "                for key in input.keys():\n",
    "                    line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"input\", key]\n",
    "                    writer.writerow(line)\n",
    "            for output in outputs:\n",
    "                for key in output.keys():\n",
    "                    line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"output\", key]\n",
    "                    writer.writerow(line)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORT memroy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_file = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.txt\"\n",
    "csv_file = mem_file.replace(\"txt\", 'csv')\n",
    "\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"Tensor name\",\"op_type\",\"Index\",\"Reuse inplace\",\"Reused Node index\",\"Alloc type\",\"Device type\",\"Memory type\",\"Device id\", \"lifetime start\",\"lifetime end\", \"planned block start\",\"planned block end\",\"planned size\", \"allocated block start\",\"allocated block end\", \"allocated size\"]\n",
    "writer.writerow(line)\n",
    "init_count = 0\n",
    "with open(mem_file,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if \"Initializer in Device\" in line:\n",
    "            init_count += 1        \n",
    "        if init_count == 3:\n",
    "            break\n",
    "        if \"Tensor name\" not in line:\n",
    "            continue\n",
    "        tensor_name = line.split(\"Tensor name: \")[1].split(\", Index\")[0].strip()\n",
    "      \n",
    "        index = line.split(\"Index: \")[1].split(\", Reuse inplace\")[0].strip()\n",
    "        reuse = line.split(\"Reuse inplace: \")[1].split(\", Reused Node index\")[0].strip()\n",
    "        reuse_index = line.split(\"Reused Node index: \")[1].split(\", Alloc type\")[0].strip()\n",
    "        alloc_type= line.split(\"Alloc type: \")[1].split(\", Location\")[0].strip()\n",
    "        if alloc_type ==\"AllocateStatically\":\n",
    "            op_type = \"\"\n",
    "        else:\n",
    "            op_type = tensor_name.split(\"/\")[-1].split(\"_\")[0] # this op is still incorrect\n",
    "        location = line.split(\"Location: \")[1].split(\", lifetime\")[0].strip()\n",
    "        device_type = location.split(\"DeviceType:\")[1].split(\"MemoryType\")[0].strip()\n",
    "        memory_type = location.split(\"MemoryType:\")[1].split(\"DeviceId\")[0].strip()\n",
    "        device_id = location.split(\"DeviceId:\")[1].split(\"]\")[0].strip()\n",
    "        \n",
    "        lt_start = line.split(\"lifetime: (\")[1].split(\",\")[0].strip()\n",
    "        lt_end = line.split(\"lifetime: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        pb_start = line.split(\"planned block: (\")[1].split(\",\")[0].strip()\n",
    "        pb_end = line.split(\"planned block: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        pb_size = line.split(\"planned size: \")[1].split(\", allocated block\")[0].strip()\n",
    "        ab_start = line.split(\"allocated block: (\")[1].split(\",\")[0].strip()\n",
    "        ab_end = line.split(\"allocated block: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        ab_size = line.split(\"allocated size: \")[1].strip()\n",
    "        newline = [tensor_name,op_type,index,reuse,reuse_index,alloc_type,device_type,memory_type,device_id,lt_start,lt_end,pb_start,pb_end,pb_size, ab_start,ab_end,ab_size]\n",
    "       \n",
    "        # strings =[tensor_name,index,reuse,alloc_type,location,lt_start,lt_end,pb_start,pb_end,pb_size, ab_start,ab_end,ab_size]\n",
    "        # newline = \",\".join(strings)\n",
    "        writer.writerow(newline)\n",
    "csvf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time file is for node, cannot mapping with memory info(output tensor)\n",
    "# combine optimized_onnx_node with memory info\n",
    "onnx_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_optimized_onnx_node.csv'\n",
    "memory_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.csv'\n",
    "new_csv = memory_csv.replace(\".csv\",\"_new.csv\")\n",
    "csvf = open(new_csv,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "count = 0\n",
    "with open(memory_csv, 'r') as mem_file:\n",
    "    reader = csv.reader(mem_file)\n",
    "    for i, mem_row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            mem_row.append(\"op_type\")\n",
    "            #mem_row.append(\"shape\")\n",
    "            writer.writerow(mem_row)\n",
    "            continue\n",
    "\n",
    "        with open(onnx_csv, 'r') as onnx_file:\n",
    "            reader2 = csv.reader(onnx_file)\n",
    "            for j, row in enumerate(reader2):\n",
    "                if j ==0:\n",
    "                    continue\n",
    "                if row[2]==\"output\" and row[3] == mem_row[0]:\n",
    "                    mem_row.append(row[1])\n",
    "                    #mem_row.append(row[2])\n",
    "                    count+=1\n",
    "                    break\n",
    "                # if j == 921 and mem_row[4] != \"AllocateStatically\":\n",
    "                #     pass\n",
    "                    #print(mem_row[0],mem_row[-1], row[0],row[2])\n",
    "        writer.writerow(mem_row)\n",
    "csvf.close()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare uent_time_all.csv with dml_node.csv\n",
    "time_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\dynamic_dim\\unet_time_all.csv'\n",
    "dml_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_dml_node.csv'\n",
    "\n",
    "node_set =set()\n",
    "\n",
    "# row_count = sum(1 for row in dmlreader)\n",
    "count = 0\n",
    "total_memory_size = 0\n",
    "max_memory_size = 0\n",
    "with open(time_csv, 'r') as timef:\n",
    "    reader = csv.reader(timef)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i==0 or len(row) == 0:\n",
    "            continue\n",
    "        node_set.add(row[0])\n",
    "        with open(dml_csv, 'r') as dmlf:\n",
    "            dmlreader = csv.reader(dmlf)\n",
    "            for j, dmlrow in enumerate(dmlreader):\n",
    "                if j==0 or len(dmlrow) ==0:\n",
    "                    continue\n",
    "                if row[0] == dmlrow[0]:\n",
    "                    count+=1\n",
    "                    shape = row[3].split(\",\")\n",
    "                    if len(shape) == 1:\n",
    "                        continue\n",
    "                    memory_size = 1\n",
    "                    for v in shape:\n",
    "                        if \"[\" in v:\n",
    "                            v_int = int(v.split(\"[\")[-1].strip())\n",
    "                        else:\n",
    "                            if \"]\" in v:\n",
    "                                v_int = int(v.split(\"]\")[0].strip())\n",
    "                            else:\n",
    "                                v_int = int(v.strip())\n",
    "                        memory_size *=v_int\n",
    "                    #print(memory_size)\n",
    "                    total_memory_size+=memory_size\n",
    "                    max_memory_size = max_memory_size if max_memory_size > memory_size else memory_size\n",
    "                    break\n",
    "                if j == 920:                   \n",
    "                    print(row[0], dmlrow[0])\n",
    "print(len(node_set)-count)\n",
    "print(total_memory_size*2/1000000000)\n",
    "print(max_memory_size*2/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_size(node_name, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        max_size = 0\n",
    "        for i in range(len(data)):\n",
    "            if \"provider\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            provider = data[i][\"args\"][\"provider\"]\n",
    "            op_name = data[i][\"args\"][\"op_name\"]\n",
    "            if provider !=\"DmlExecutionProvider\":\n",
    "                continue\n",
    "                # for value in data[i][\"args\"][\"input_type_shape\"]:\n",
    "                #     if \"float\" in value:\n",
    "                #         print(data[i][\"name\"], value)\n",
    "            if node_name in op_name:\n",
    "                for value in data[i][\"args\"][\"input_type_shape\"]:\n",
    "                    if \"float16\" in value: # float16 = 2B\n",
    "\n",
    "                        x_shape = np.array(value[\"float16\"])\n",
    "                        temp_size = np.prod(x_shape)#[0]*x_shape[1]*x_shape[2]        \n",
    "                        if max_size < temp_size:\n",
    "                            max_size =  temp_size\n",
    "                            kernel_name = data[i][\"name\"]\n",
    "    print(max_size)\n",
    "    print(kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__unet_1.5_olive.json'\n",
    "#get_max_size(\"LayerNormalization\")\n",
    "#get_max_size(\"InstanceNormalization\")\n",
    "get_max_size(\"MatMul\",filename)\n",
    "#get_max_size(\"Mul\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(provider,op, path):\n",
    "    total_time = 0\n",
    "    provider_time = 0\n",
    "    ops_time = {}\n",
    "    with open(path, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            if \"op_name\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            op_name = data[i][\"args\"][\"op_name\"]               \n",
    "            duration = int(data[i][\"dur\"])\n",
    "            total_time +=duration\n",
    "            if \"provider\" in data[i][\"args\"]:\n",
    "                provider_name = data[i][\"args\"][\"provider\"]\n",
    "                if provider == provider_name:\n",
    "                    if op == op_name or op == None:\n",
    "                        provider_time +=duration\n",
    "                        if \"DmlFusedNode\" in op_name:\n",
    "                            continue\n",
    "                        if op_name in ops_time:\n",
    "                            ops_time[op_name] += duration\n",
    "                        else:\n",
    "                            ops_time[op_name] = 0\n",
    "\n",
    "                        \n",
    "            \n",
    "    print(total_time)\n",
    "    print(ops_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_time(\"DmlExecutionProvider\",\"MemcpyFromHost\", path)\n",
    "get_time(\"DmlExecutionProvider\",\"MemcpyToHost\", path)\n",
    "get_time(\"DmlExecutionProvider\",None, path)\n",
    "get_time(\"CPUExecutionProvider\",None, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernelname(provider, op,filename):\n",
    "    ops = set()\n",
    "    with open(path, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            if \"op_name\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            op_name = data[i][\"args\"][\"op_name\"]               \n",
    "            if \"provider\" in data[i][\"args\"]:\n",
    "                provider_name = data[i][\"args\"][\"provider\"]\n",
    "                if provider == provider_name:\n",
    "                    if op == op_name or op == None:\n",
    "                        print(data[i]['name'])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_kernelname(\"CPUExecutionProvider\",\"Mul\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(filename):\n",
    "    input_data_type = set()\n",
    "    output_data_type = set()\n",
    "    with open(filename, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        max_size = 0\n",
    "        for i in range(len(data)):\n",
    "            if \"provider\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            \n",
    "            inputs = data[i][\"args\"][\"input_type_shape\"]\n",
    "            outputs = data[i][\"args\"][\"output_type_shape\"]\n",
    "            for input in inputs:\n",
    "                for key in input.keys():\n",
    "                    input_data_type.add(key)\n",
    "            for output in outputs:\n",
    "                for key in output.keys():\n",
    "                    output_data_type.add(key)\n",
    "    print(input_data_type)\n",
    "    print(output_data_type)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_type(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze openvino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(r'..\\AIGC\\optimize\\unet_ov.xml')\n",
    "root = tree.getroot()\n",
    "for child in root:\n",
    "    if child.tag !=\"layers\":\n",
    "        continue\n",
    "    for subchild in child:\n",
    "        if \"FullyConnected\" == subchild.attrib[\"type\"]:\n",
    "            ports = subchild.find(\"input/port\")\n",
    "            if len(ports) == 2:\n",
    "                print(subchild.attrib[\"name\"])\n",
    "                break\n",
    "            # data_size = 1\n",
    "            # for value in ports.iter(\"dim\"):\n",
    "            #     data_size *= int(value.text)\n",
    "            # print(data_size)\n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert onnx memory file into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_file = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.txt\"\n",
    "csv_file = mem_file.replace(\"txt\", 'csv')\n",
    "\n",
    "\n",
    "with open(mem_file,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if \"peak_rss\" not in line:\n",
    "            continue\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandasForSortingCSV\n",
    "def get_graph_info(file_csv,idx):\n",
    "    # assign dataset\n",
    "    csvData = pandasForSortingCSV.read_csv(file_csv)\n",
    "    print(csvData.groupby(csvData.columns[idx]).sum() )\n",
    "                                         \n",
    "    # sort data frame\n",
    "    # csvData.sort_values(csvData.columns[1], \n",
    "    #                     axis=0,\n",
    "    #                     inplace=True)                    \n",
    "    \n",
    "    # # displaying sorted data frame\n",
    "    # print(\"\\nAfter sorting:\")\n",
    "    # print(csvData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
