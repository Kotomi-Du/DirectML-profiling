{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Adobe model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = \"model_mapping.txt\"\n",
    "line_count = 0\n",
    "dict_name={}\n",
    "with open(mapping_file, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line_count+=1\n",
    "        if line_count == 1:\n",
    "            continue\n",
    "        a,b=line.rstrip().split(\"\t\")\n",
    "        dict_name[b] = a\n",
    "\n",
    "print(dict_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all onnx model\n",
    "import os\n",
    "import shutil\n",
    "# Provide the directory path\n",
    "directory = r\"Adobe_models\\Adobe_models\\AI_Model_Restricted\"\n",
    "new_directory = r\"model_rename\"\n",
    "# Traverse through the directory\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    \n",
    "    for file in files:\n",
    "        # Print the file name\n",
    "        key = file.split(\".onnx\")[0] \n",
    "        if key in dict_name.keys():\n",
    "\n",
    "            target_path = os.path.join(new_directory, dict_name[key]+\".onnx\")\n",
    "            source_path = os.path.join(root,file)\n",
    "            if not os.path.exists(source_path):\n",
    "                print(source_path)\n",
    "                continue\n",
    "            shutil.copy2(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract DDI info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "ddi_root= r\"C:\\Intel\\igfx\\d3d12\"\n",
    "new_root= r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_backup\"\n",
    "modelfile = glob.glob(r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\model_rename\\*.onnx\")\n",
    "#print(modelfile)\n",
    "for i in range(45):\n",
    "        ddi_file = str(i+1)+\".log\"\n",
    "        print(os.path.basename(modelfile[i]))\n",
    "        modelname = os.path.basename(modelfile[i]).split(\".onnx\")[0]\n",
    "        source_path = os.path.join(ddi_root, ddi_file)\n",
    "        target_path = os.path.join(new_root, modelname+\".log\")\n",
    "        if not os.path.exists(source_path):\n",
    "            print(source_path)\n",
    "            continue\n",
    "        shutil.copy(source_path, target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "input_string = \"Input = [0x1,0x200,0x80,0x80], Filter = [0x200,0x200,0x3,0x3], Bias = [0x1,0x200,0x1,0x1], Output = [0x1,0x200,0x80,0x80]\"\n",
    "input_string = \"Input = [0x1,0x3,0x400,0x400], Filter = [0x80,0x3,0x3,0x3], Bias = [0x1,0x80,0x1,0x1], Output = [0x1,0x80,0x400,0x400]\"\n",
    "values = re.findall(r'0x\\w+', input_string)\n",
    "output = []\n",
    "for idx, val in enumerate(values):\n",
    "    output.append(int(val, 16))\n",
    "\n",
    "print(\"input \", output[:4])\n",
    "print(\"filter\", output[4:8])\n",
    "print(\"bias  \", output[8:12])\n",
    "print(\"output\", output[12:16])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse ddi log into json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def create_ddi_json(root_path,filename):\n",
    "    ddi_file = os.path.join(root_path, filename)\n",
    "    json_file = os.path.join(root_path, filename.replace(\"log\",\"json\"))\n",
    "    dic ={}\n",
    "    openf =  open(ddi_file, \"r\")\n",
    "\n",
    "    lines = openf.readlines()\n",
    "    #print(len(lines))\n",
    "    i = -1\n",
    "    while i < len(lines)-1:\n",
    "        i+=1\n",
    "        if \"Passed-Metacommand type : Convolution1\" in lines[i]:\n",
    "            #print(lines[i-12:i])\n",
    "            mc_type = lines[i].rstrip().split(\"type :\")[-1]\n",
    "            \n",
    "            kernel_name_idx = -1\n",
    "            for k in range(11):\n",
    "                if \"Conv Kernel:\" in lines[i-k]:\n",
    "                    kernel_name_idx = i-k\n",
    "                    # print(lines[kernel_name_idx])\n",
    "                    # print(kernel_name_idx)\n",
    "                    break\n",
    "                if k == 10:\n",
    "                    assert(\"no conv kernel\")\n",
    "\n",
    "            kernel_name = lines[kernel_name_idx].rstrip().split(\"Kernel:\")[-1]\n",
    "            #print(mc_type, kernel_name)\n",
    "            if mc_type not in dic.keys():\n",
    "                dic[mc_type] = {}\n",
    "            if kernel_name not in dic[mc_type].keys():\n",
    "                dic[mc_type][kernel_name] = []\n",
    "            \n",
    "            layout_idx = -1\n",
    "            dim_idx = -1\n",
    "            param_str = []\n",
    "            for k in range(6):\n",
    "                dim_idx = kernel_name_idx-k\n",
    "                layout_idx = kernel_name_idx - k+2\n",
    "                param_str =  re.findall(r'0x\\w+', lines[dim_idx])\n",
    "                if len(param_str) > 5:\n",
    "                    break\n",
    "            \n",
    "          \n",
    "            # print(dim_idx, lines[dim_idx])   \n",
    "            # print(param_str)\n",
    "            param_list = []\n",
    "            for idx, val in enumerate(param_str):\n",
    "                param_list.append(int(val, 16))\n",
    "\n",
    "            layout_str = \"\"\n",
    "            #print(lines[layout_idx].rstrip(), layout_idx,kernel_name_idx)\n",
    "            layout_str = re.findall(r'N\\w+', lines[layout_idx])\n",
    "\n",
    "            ''' some log  BiasDesc = isNull'''\n",
    "            output_shape = []\n",
    "            if \"IsNull\" in lines[i+5+38]:\n",
    "                outputdesc_idx = i+5+38+2\n",
    "                output_shape = param_list[9:13]\n",
    "            else:\n",
    "                outputdesc_idx = i+5+19+38\n",
    "                output_shape =  param_list[13:17]\n",
    "\n",
    "            input_stride = []\n",
    "            for j in range(4):\n",
    "                input_stride.append(int(lines[i+12+j].rstrip().split(\"=\")[-1],16))\n",
    "            output_stride = []\n",
    "            for j in range(4):\n",
    "                output_stride.append(int(lines[outputdesc_idx+8+j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            # inputpadding_list = \n",
    "            outputpadding_list = []\n",
    "            for j in range(5):\n",
    "                outputpadding_list.append(lines[outputdesc_idx+35+j].rstrip().split(\"=\")[-1])\n",
    " \n",
    "            filter_stride = []\n",
    "            for j in range(3):\n",
    "                filter_stride.append(int(lines[outputdesc_idx+22+j].rstrip().split(\"=\")[-1],16))\n",
    "\n",
    "            filter_dilation =  []\n",
    "            for j in range(3):\n",
    "                filter_dilation.append(int(lines[outputdesc_idx+25+j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "            input_padding = []\n",
    "            for j in range(6):\n",
    "                input_padding.append(int(lines[outputdesc_idx+28+j].rstrip().split(\"=\")[-1],16))\n",
    "            \n",
    "\n",
    "            \n",
    "            group_count =int( re.findall(r'0x\\w+', lines[outputdesc_idx+40])[0],16)\n",
    "            info_dic ={}\n",
    "            info_dic[\"input\"] = {\"shape\": param_list[1:5], \"layout\": layout_str[0], \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5])[0], \"flag\": re.findall(r'\\((.*?)\\)', lines[i+6])[0], \"stride\": input_stride, \"padding\": input_padding}\n",
    "            info_dic[\"filter\"] = {\"shape\": param_list[5:9], \"layout\": layout_str[1], \"datatype\": re.findall(r'\\((.*?)\\)', lines[i+5 + 19])[0],\"flag\": re.findall(r'\\((.*?)\\)', lines[i+6+19])[0],\"stirde\": filter_stride, \"dilation\":filter_dilation,  \"groupcount\": group_count}\n",
    "            info_dic[\"output\"] = {\"shape\":output_shape, \"layout\": layout_str[2], \"datatype\": re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+1])[0],\"flag\":re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+2])[0], \"stride\": output_stride,\"padding\":outputpadding_list}\n",
    "            info_dic[\"direction\"] = re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+20])[0]\n",
    "            \n",
    "            if \"Function\" in lines[outputdesc_idx+42]:\n",
    "                info_dic[\"activation\"] =  re.findall(r'\\((.*?)\\)', lines[outputdesc_idx+42])[0]\n",
    "            else:\n",
    "                info_dic[\"activation\"] = \"isnull\"\n",
    "            dic[mc_type][kernel_name].append(info_dic)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    json_object = json.dumps(dic, indent=4)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "root_path = r\"C:\\Intel\\igfx\\d3d12\"\n",
    "basename = \"onnxruntime_perf_test0.log\"\n",
    "create_ddi_json(root_path, basename)\n",
    "\n",
    "# root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "# files = glob.glob(root_path+\"\\\\*.log\")\n",
    "\n",
    "# for file in files:\n",
    "#     basename = os.path.basename(file)\n",
    "#     create_ddi_json(root_path, basename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_1x1_nhwc_ob32_maxbw_fp16 92\n",
      " conv_3x3_nhwc_ob32_fp16_withMask_wLeftOver 14\n",
      " g_BFYX_OS_IYX_OSV16_OCL 36\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Intel\\igfx\\d3d12\"\n",
    "basename = \"onnxruntime_perf_test0.json\"\n",
    "count = 0\n",
    "\n",
    "df = pd.read_json(os.path.join(root_path,basename))\n",
    "#print(basename)\n",
    "str_info = basename+\"\t\"\n",
    "conv =df[df.keys()[0]]\n",
    "\n",
    "for key in conv.keys():\n",
    "    print(key, len(conv[key]))\n",
    "    #for i in range(len(conv[key])):\n",
    "\n",
    "\n",
    "#print(count) \n",
    "    #print(str_info,f32_count,f16_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert json to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "root_path = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\ddilog_full\"\n",
    "files = glob.glob(root_path+\"\\\\*.json\")\n",
    "\n",
    "csv_file = os.path.join(root_path,\"adobe_conv_ddilog.csv\")\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer =csv.writer(csvf)\n",
    "line = [\"model_name\", \"kernel_name\",\"input_shape_n\",\"input_shape_c\",\"input_shape_h\",\"input_shape_w\", \"input_layout\", \"input_datatype\",\"input_flag\",\"input_padding\",\\\n",
    "        \"filter_shape_n\",\"filter_shape_c\",\"filter_shape_h\",\"filter_shape_w\", \"filter_layout\", \"filter_datatype\",\"filter_flag\", \"filter_stride_h\", \"filter_stride_w\", \"filter_stride_c\", \"filter_dilation_h\", \"filter_dilation_w\", \"filter_dilation_c\", \"filter_groupcount\",\n",
    "        \"output_shape_n\",\"output_shape_c\",\"output_shape_h\",\"output_shape_w\", \"output_layout\", \"output_datatype\",\"output_flag\", \"output_padding\", \n",
    "         \"direction\", \"activation\" ]\n",
    "writer.writerow(line)\n",
    "for file in files:\n",
    "    basename = os.path.basename(file)\n",
    "    df = pd.read_json(file)\n",
    "    conv =df[df.keys()[0]]\n",
    "    for key in conv.keys():\n",
    "        kernel_name = key\n",
    "        for i in range(len(conv[key])):\n",
    "            input_shape = conv[key][i][\"input\"][\"shape\"]\n",
    "            input_layout = conv[key][i][\"input\"][\"layout\"]\n",
    "            input_datatype = conv[key][i][\"input\"][\"datatype\"]\n",
    "            input_flag = conv[key][i][\"input\"][\"flag\"]\n",
    "            input_padding = [int(v) for v in  conv[key][i][\"input\"][\"padding\"]] \n",
    "\n",
    "            filter_shape = conv[key][i][\"filter\"][\"shape\"]\n",
    "            filter_layout = conv[key][i][\"filter\"][\"layout\"]\n",
    "            filter_datatype = conv[key][i][\"filter\"][\"datatype\"]\n",
    "            filter_flag = conv[key][i][\"filter\"][\"flag\"]\n",
    "            filter_stride = conv[key][i][\"filter\"][\"stirde\"]\n",
    "            filter_dilation = conv[key][i][\"filter\"][\"dilation\"]\n",
    "            filter_groupcount = conv[key][i][\"filter\"][\"groupcount\"]\n",
    "\n",
    "            output_shape = conv[key][i][\"output\"][\"shape\"]\n",
    "            output_layout = conv[key][i][\"output\"][\"layout\"]\n",
    "            output_datatype = conv[key][i][\"output\"][\"datatype\"]\n",
    "            output_flag = conv[key][i][\"output\"][\"flag\"]\n",
    "            output_padding = [int(v) for v in conv[key][i][\"output\"][\"padding\"]]\n",
    "\n",
    "            direction = conv[key][i][\"direction\"]\n",
    "            activation = conv[key][i][\"activation\"]\n",
    "\n",
    "            writer.writerow([basename, kernel_name, input_shape[0], input_shape[1], input_shape[2], input_shape[3], input_layout, input_datatype, input_flag,input_padding,\\\n",
    "                              filter_shape[0],filter_shape[1],filter_shape[2],filter_shape[3], filter_layout, filter_datatype, filter_flag, filter_stride[0], filter_stride[1],filter_stride[2],filter_dilation[0],filter_dilation[1],filter_dilation[2], filter_groupcount, \\\n",
    "                                output_shape[0],output_shape[1],output_shape[2],output_shape[3], output_layout, output_datatype, output_flag, output_padding,\n",
    "                                direction, activation])\n",
    "    \n",
    "csvf.close()\n",
    "            \n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract PIX info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "rootpath = r\"C:\\Users\\GAME\\Documents\\Project\"\n",
    "log_file = \"PIX_log.txt\"\n",
    "file = os.path.join(rootpath, log_file)\n",
    "file = file.strip(\".txt\")\n",
    "rawdata = pd.read_csv(f\"{file}.txt\",delimiter = '\\t')\n",
    "\n",
    "signal_count = 0\n",
    "first_iteration_start = 0\n",
    "second_iteration_start = 0  \n",
    "## Option 1:\n",
    "#  use \"DML_EXECUTION_PLAN\" to find where is the start of first_iteration and second iteration\n",
    "#  value = line number - 1\n",
    "#  \n",
    "## Option 2:\n",
    "#  use the algorithm below to find first iteration and second iteration\n",
    "#  but mostly the time it does not work\n",
    "'''\n",
    "for index, line in rawdata.iterrows():\n",
    "    if signal_count == 5:\n",
    "        first_iteration = index\n",
    "    if signal_count == 10:\n",
    "        second_iteration = index\n",
    "        break\n",
    "    if \"Signal\" in line :\n",
    "        signal_count +=0\n",
    "'''\n",
    "## Option 3: [todo] use the whole information to decide\n",
    "\n",
    "first_iteration_start = 180\n",
    "second_iteration_start = 960\n",
    "prevline = \"\"\n",
    "ex_operator_list=[]\n",
    "ex_time = []\n",
    "\n",
    "dispatch_operator_list=[]\n",
    "dispatch_time=[]\n",
    "\n",
    "pre_checkDispatch =False\n",
    "idx = -1\n",
    "while idx < len(rawdata)-2: \n",
    "    idx+=1\n",
    "    if idx < first_iteration_start:\n",
    "        continue\n",
    "    if idx > second_iteration_start:\n",
    "        break\n",
    "    line = rawdata.iloc[idx]\n",
    "    if \"ExecuteMetaCommand\" in line[2]:      \n",
    "        ex_operator_list.append((prevline[2].strip()))\n",
    "        ex_time.append(int(prevline[4]))\n",
    "        # temp.append(line)\n",
    "    if \"Dispatch\" in line[2]:\n",
    "        if pre_checkDispatch:\n",
    "            continue\n",
    "        dispatch_operator_list.append((prevline[2].strip()))\n",
    "        dispatch_time.append(int(prevline[4]))\n",
    "        pre_checkDispatch = True \n",
    "    else:\n",
    "        prevline = line\n",
    "        pre_checkDispatch = False\n",
    "\n",
    "sumup= (sum(dispatch_time) + sum(ex_time))/1000000\n",
    "print(\"total latency per iteration: {} ms \\n \\\n",
    "      Tip: if the data is too different from ort_perf_test.exe,\\n \\\n",
    "      please double check the first/second iteration number\".format(round(sumup,2)))\n",
    "\n",
    "\n",
    "csv_file = f\"{file}_test.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"execute type\",\"layer type\",\"layer name\",\"time\"]\n",
    "writer.writerow(line)\n",
    "\n",
    "for op,time in zip(ex_operator_list,ex_time):\n",
    "    layer_info = op.split(\",\")[-1]\n",
    "    mark_idx = layer_info.index(\"(\")\n",
    "    layer_type = layer_info[0:mark_idx-1]\n",
    "    layer_name = layer_info[mark_idx+1:-1]\n",
    "    writer.writerow([\"ExecuteMetaCommand\",layer_type, layer_name,round(float(time)/1000000,2)])\n",
    "\n",
    "for op,time in zip(dispatch_operator_list,dispatch_time):\n",
    "    layer_info = op.split(\",\")[-1]\n",
    "    mark_idx = layer_info.index(\"(\")\n",
    "    layer_type = layer_info[0:mark_idx-1]\n",
    "    layer_name = layer_info[mark_idx+1:-1]\n",
    "    writer.writerow([\"Dispatch\",layer_type, layer_name,round(float(time)/1000000,2)])\n",
    "\n",
    "\n",
    "csvf.close()\n",
    "print(\"{} generated\".format(csv_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract info from benchmark.bat log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DML backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"dml_log_release.txt\"\n",
    "csv_file = \"dml_perf_release.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"model name\",\"not save model\",\"save model\"]\n",
    "writer.writerow(line)\n",
    "with open(log_file,\"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    idx = 0\n",
    "    while idx < len(all_lines):\n",
    "        line = all_lines[idx]\n",
    "        if \"model_rename\" in line:\n",
    "            model_name = line.rstrip().split(\"model_rename\\\\\")[-1]\n",
    "            if \"Average inference\" not in all_lines[idx+6]:\n",
    "                perf_save = all_lines[idx+10].rstrip().split(\"time cost:\")[-1]  \n",
    "                writer.writerow([model_name, \"erro\", perf_save])\n",
    "                idx = idx + 25\n",
    "                print(idx)\n",
    "            else:\n",
    "                perf_nosave = all_lines[idx+6].rstrip().split(\"time cost:\")[-1]   \n",
    "                perf_save = all_lines[idx+26].rstrip().split(\"time cost:\")[-1]\n",
    "                writer.writerow( [model_name , perf_nosave, perf_save])\n",
    "                idx = idx+41\n",
    "csvf.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"ov_log.txt\"\n",
    "csv_file = \"ov_perf.csv\"\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"model name\",\"FP16\",\"FP32\"]\n",
    "writer.writerow(line)\n",
    "with open(log_file,\"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    idx = 0\n",
    "    while idx < len(all_lines):\n",
    "        line = all_lines[idx]\n",
    "        if \"model_rename\" in line:\n",
    "            model_name = line.rstrip().split(\"model_rename\\\\\")[-1]\n",
    "            print(model_name)\n",
    "            if \"time cost\" not in all_lines[idx+6]:\n",
    "                perf_save = all_lines[idx+7].rstrip().split(\"time cost:\")[-1]  \n",
    "                writer.writerow([model_name, \"erro\", perf_save])\n",
    "                idx = idx + 22\n",
    "            else:\n",
    "                perf_nosave = all_lines[idx+6].rstrip().split(\"time cost:\")[-1]   \n",
    "                perf_save = all_lines[idx+26].rstrip().split(\"time cost:\")[-1]\n",
    "                writer.writerow( [model_name , perf_nosave, perf_save])\n",
    "                idx = idx+41\n",
    "csvf.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract OV performance data\n",
    "## Option 1 if you have ort_perf_test.exe debug version:\n",
    "1. set `ORT_OPENVINO_ENABLE_DEBUG=1`, and run ort_perf_test.exe with ORT_OV backend\n",
    "2. some performance logs will be appeared in the command window, need to manually save it and process into csv file [The script below is some reference but it is not very good]\n",
    "## Option 2\n",
    "1. set python environment: pip install openvino_dev\n",
    "2. this package contains a tool called benchmark_app\n",
    "3. run benchmark_app with the command line\n",
    "`benchmark_app -m your_onnx_model_path -d GPU -nireq 1 -niter 10 --report_type detailed_counters --report_folder perf\\`\n",
    "4. after running this, a performance csv file will be generated under `perf\\` folder\n",
    "\n",
    "\n",
    "> I think the performance result in Option2 is validated for profiling ORT_OV performance because  I have checked the pipeline in onnxruntime ov. From my understanding, ort only did some work about parsing onnx model and rest of the works are all handled by OV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_file = r\"C:\\Users\\GAME\\Documents\\Project\\Adobe\\analyze\\Adobe-Ps_SuperZoom_V316.ov.txt\"\n",
    "csv_file = perf_file.replace(\"txt\",'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"layer_type\",\"layer name\",\"gpu(ms)\",\"cpu(ms)\"]\n",
    "writer.writerow(line)\n",
    "total_time = 0\n",
    "with open(perf_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line_items = line.rstrip().split(\" \")\n",
    "        info = [i for i in line_items if i != '']\n",
    "        #print(info)\n",
    "        layer_type = info[3]\n",
    "        layer_name = info[0]\n",
    "        gpu_time = int(info[5])/1000\n",
    "        cpu_time = int(info[7])/1000\n",
    "        total_time+=gpu_time\n",
    "        if gpu_time == 0.0:\n",
    "            continue\n",
    "        writer.writerow([layer_type,layer_name, gpu_time,cpu_time])\n",
    "print(\"total time: {:2f}\".format(total_time)) \n",
    "csvf.close()     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract info from ORT profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORT time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_file = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_time_all.json'\n",
    "csv_file = time_file.replace(\"json\", 'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"start\",\"duration\",\"name\",\"op_name\", \"provider\", \"role\", \"data_type\"]\n",
    "writer.writerow(line)\n",
    "with open(time_file, 'r') as f:\n",
    "    # load the contents of the file into a dictionary\n",
    "    data = json.load(f)\n",
    "    for i in range(len(data)):\n",
    "        temp = \"\"\n",
    "        if \"op_name\" not in data[i][\"args\"]:\n",
    "            continue\n",
    "        if \"provider\" not in data[i][\"args\"]:\n",
    "            temp = \"none\"\n",
    "            data_type = \"none\"\n",
    "            line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"none\", data_type]\n",
    "            writer.writerow(line)\n",
    "        else:\n",
    "            temp = data[i][\"args\"][\"provider\"]\n",
    "            inputs = data[i][\"args\"][\"input_type_shape\"]\n",
    "            outputs = data[i][\"args\"][\"output_type_shape\"]\n",
    "            for input in inputs:\n",
    "                for key in input.keys():\n",
    "                    line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"input\", key]\n",
    "                    writer.writerow(line)\n",
    "            for output in outputs:\n",
    "                for key in output.keys():\n",
    "                    line = [data[i][\"ts\"],data[i][\"dur\"],data[i][\"name\"],data[i][\"args\"][\"op_name\"], temp, \"output\", key]\n",
    "                    writer.writerow(line)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "csvf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape\n",
    "# index value in profile is same with allocator_planner.cc\n",
    "# index value in tensor allocation is different, where is it?\n",
    "time_file = r'C:\\Users\\GAME\\Documents\\Project\\onnxruntime\\build\\Windows\\Debug\\Debug\\file_small.log_2023-06-20_15-08-04.json'\n",
    "csv_file = time_file.replace(\"json\", 'csv')\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"name\",\"op_type\",\"output_size\",\"output_shape\"]\n",
    "writer.writerow(line)\n",
    "count = 0\n",
    "node_count =0\n",
    "with open(time_file, 'r') as f:\n",
    "    # load the contents of the file into a dictionary\n",
    "    data = json.load(f)\n",
    "    for i in range(len(data)):\n",
    "        if \"/conv_in/Conv_fence_before\" in data[i][\"name\"]:\n",
    "            count += 1\n",
    "        if count == 2:\n",
    "            break\n",
    "        if \"kernel\" in data[i][\"name\"]:\n",
    "           \n",
    "            output_name = data[i][\"name\"].split(\"_kernel\")[0]\n",
    "            op_type = data[i]['args']['op_name']\n",
    "            #print(output_name)\n",
    "            output_size = data[i]['args']['output_size']\n",
    "            #print(data[i]['args']['input_type_shape'])\n",
    "            if \"float16\" in data[i]['args']['output_type_shape'][0].keys():\n",
    "                output_shape = data[i]['args']['output_type_shape'][0][\"float16\"] # only one output\n",
    "            else:\n",
    "                continue\n",
    "            # if \"int64\" in data[i]['args']['output_type_shape'][0].keys():\n",
    "            #     output_shape = data[i]['args']['output_type_shape'][0][\"int64\"]\n",
    "            newline = [output_name, op_type, output_size,output_shape]\n",
    "            writer.writerow(newline)\n",
    "            node_count+=1\n",
    "print(node_count)\n",
    "csvf.close()\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORT memroy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_file = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.txt\"\n",
    "csv_file = mem_file.replace(\"txt\", 'csv')\n",
    "\n",
    "csvf = open(csv_file,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "line = [\"Tensor name\",\"op_type\",\"Index\",\"Reuse inplace\",\"Reused Node index\",\"Alloc type\",\"Device type\",\"Memory type\",\"Device id\", \"lifetime start\",\"lifetime end\", \"planned block start\",\"planned block end\",\"planned size\", \"allocated block start\",\"allocated block end\", \"allocated size\"]\n",
    "writer.writerow(line)\n",
    "init_count = 0\n",
    "with open(mem_file,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if \"Initializer in Device\" in line:\n",
    "            init_count += 1        \n",
    "        if init_count == 3:\n",
    "            break\n",
    "        if \"Tensor name\" not in line:\n",
    "            continue\n",
    "        tensor_name = line.split(\"Tensor name: \")[1].split(\", Index\")[0].strip()\n",
    "      \n",
    "        index = line.split(\"Index: \")[1].split(\", Reuse inplace\")[0].strip()\n",
    "        reuse = line.split(\"Reuse inplace: \")[1].split(\", Reused Node index\")[0].strip()\n",
    "        reuse_index = line.split(\"Reused Node index: \")[1].split(\", Alloc type\")[0].strip()\n",
    "        alloc_type= line.split(\"Alloc type: \")[1].split(\", Location\")[0].strip()\n",
    "        if alloc_type ==\"AllocateStatically\":\n",
    "            op_type = \"\"\n",
    "        else:\n",
    "            op_type = tensor_name.split(\"/\")[-1].split(\"_\")[0] # this op is still incorrect\n",
    "        location = line.split(\"Location: \")[1].split(\", lifetime\")[0].strip()\n",
    "        device_type = location.split(\"DeviceType:\")[1].split(\"MemoryType\")[0].strip()\n",
    "        memory_type = location.split(\"MemoryType:\")[1].split(\"DeviceId\")[0].strip()\n",
    "        device_id = location.split(\"DeviceId:\")[1].split(\"]\")[0].strip()\n",
    "        \n",
    "        lt_start = line.split(\"lifetime: (\")[1].split(\",\")[0].strip()\n",
    "        lt_end = line.split(\"lifetime: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        pb_start = line.split(\"planned block: (\")[1].split(\",\")[0].strip()\n",
    "        pb_end = line.split(\"planned block: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        pb_size = line.split(\"planned size: \")[1].split(\", allocated block\")[0].strip()\n",
    "        ab_start = line.split(\"allocated block: (\")[1].split(\",\")[0].strip()\n",
    "        ab_end = line.split(\"allocated block: (\")[1].split(\",\")[1].split(\")\")[0].strip()\n",
    "        ab_size = line.split(\"allocated size: \")[1].strip()\n",
    "        newline = [tensor_name,op_type,index,reuse,reuse_index,alloc_type,device_type,memory_type,device_id,lt_start,lt_end,pb_start,pb_end,pb_size, ab_start,ab_end,ab_size]\n",
    "       \n",
    "        # strings =[tensor_name,index,reuse,alloc_type,location,lt_start,lt_end,pb_start,pb_end,pb_size, ab_start,ab_end,ab_size]\n",
    "        # newline = \",\".join(strings)\n",
    "        writer.writerow(newline)\n",
    "csvf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time file is for node, cannot mapping with memory info(output tensor)\n",
    "# combine optimized_onnx_node with memory info\n",
    "onnx_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_optimized_onnx_node.csv'\n",
    "memory_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.csv'\n",
    "new_csv = memory_csv.replace(\".csv\",\"_new.csv\")\n",
    "csvf = open(new_csv,\"w\",newline='')\n",
    "writer = csv.writer(csvf)\n",
    "count = 0\n",
    "with open(memory_csv, 'r') as mem_file:\n",
    "    reader = csv.reader(mem_file)\n",
    "    for i, mem_row in enumerate(reader):\n",
    "        if i == 0:\n",
    "            mem_row.append(\"op_type\")\n",
    "            #mem_row.append(\"shape\")\n",
    "            writer.writerow(mem_row)\n",
    "            continue\n",
    "\n",
    "        with open(onnx_csv, 'r') as onnx_file:\n",
    "            reader2 = csv.reader(onnx_file)\n",
    "            for j, row in enumerate(reader2):\n",
    "                if j ==0:\n",
    "                    continue\n",
    "                if row[2]==\"output\" and row[3] == mem_row[0]:\n",
    "                    mem_row.append(row[1])\n",
    "                    #mem_row.append(row[2])\n",
    "                    count+=1\n",
    "                    break\n",
    "                # if j == 921 and mem_row[4] != \"AllocateStatically\":\n",
    "                #     pass\n",
    "                    #print(mem_row[0],mem_row[-1], row[0],row[2])\n",
    "        writer.writerow(mem_row)\n",
    "csvf.close()\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare uent_time_all.csv with dml_node.csv\n",
    "time_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\dynamic_dim\\unet_time_all.csv'\n",
    "dml_csv = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_dml_node.csv'\n",
    "\n",
    "node_set =set()\n",
    "\n",
    "# row_count = sum(1 for row in dmlreader)\n",
    "count = 0\n",
    "total_memory_size = 0\n",
    "max_memory_size = 0\n",
    "with open(time_csv, 'r') as timef:\n",
    "    reader = csv.reader(timef)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i==0 or len(row) == 0:\n",
    "            continue\n",
    "        node_set.add(row[0])\n",
    "        with open(dml_csv, 'r') as dmlf:\n",
    "            dmlreader = csv.reader(dmlf)\n",
    "            for j, dmlrow in enumerate(dmlreader):\n",
    "                if j==0 or len(dmlrow) ==0:\n",
    "                    continue\n",
    "                if row[0] == dmlrow[0]:\n",
    "                    count+=1\n",
    "                    shape = row[3].split(\",\")\n",
    "                    if len(shape) == 1:\n",
    "                        continue\n",
    "                    memory_size = 1\n",
    "                    for v in shape:\n",
    "                        if \"[\" in v:\n",
    "                            v_int = int(v.split(\"[\")[-1].strip())\n",
    "                        else:\n",
    "                            if \"]\" in v:\n",
    "                                v_int = int(v.split(\"]\")[0].strip())\n",
    "                            else:\n",
    "                                v_int = int(v.strip())\n",
    "                        memory_size *=v_int\n",
    "                    #print(memory_size)\n",
    "                    total_memory_size+=memory_size\n",
    "                    max_memory_size = max_memory_size if max_memory_size > memory_size else memory_size\n",
    "                    break\n",
    "                if j == 920:                   \n",
    "                    print(row[0], dmlrow[0])\n",
    "print(len(node_set)-count)\n",
    "print(total_memory_size*2/1000000000)\n",
    "print(max_memory_size*2/1000000000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_size(node_name, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        max_size = 0\n",
    "        for i in range(len(data)):\n",
    "            if \"provider\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            provider = data[i][\"args\"][\"provider\"]\n",
    "            op_name = data[i][\"args\"][\"op_name\"]\n",
    "            if provider !=\"DmlExecutionProvider\":\n",
    "                continue\n",
    "                # for value in data[i][\"args\"][\"input_type_shape\"]:\n",
    "                #     if \"float\" in value:\n",
    "                #         print(data[i][\"name\"], value)\n",
    "            if node_name in op_name:\n",
    "                for value in data[i][\"args\"][\"input_type_shape\"]:\n",
    "                    if \"float16\" in value: # float16 = 2B\n",
    "\n",
    "                        x_shape = np.array(value[\"float16\"])\n",
    "                        temp_size = np.prod(x_shape)#[0]*x_shape[1]*x_shape[2]        \n",
    "                        if max_size < temp_size:\n",
    "                            max_size =  temp_size\n",
    "                            kernel_name = data[i][\"name\"]\n",
    "    print(max_size)\n",
    "    print(kernel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__unet_1.5_olive.json'\n",
    "#get_max_size(\"LayerNormalization\")\n",
    "#get_max_size(\"InstanceNormalization\")\n",
    "get_max_size(\"MatMul\",filename)\n",
    "#get_max_size(\"Mul\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time(provider,op, path):\n",
    "    total_time = 0\n",
    "    provider_time = 0\n",
    "    ops_time = {}\n",
    "    with open(path, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            if \"op_name\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            op_name = data[i][\"args\"][\"op_name\"]               \n",
    "            duration = int(data[i][\"dur\"])\n",
    "            total_time +=duration\n",
    "            if \"provider\" in data[i][\"args\"]:\n",
    "                provider_name = data[i][\"args\"][\"provider\"]\n",
    "                if provider == provider_name:\n",
    "                    if op == op_name or op == None:\n",
    "                        provider_time +=duration\n",
    "                        if \"DmlFusedNode\" in op_name:\n",
    "                            continue\n",
    "                        if op_name in ops_time:\n",
    "                            ops_time[op_name] += duration\n",
    "                        else:\n",
    "                            ops_time[op_name] = 0\n",
    "\n",
    "                        \n",
    "            \n",
    "    print(total_time)\n",
    "    print(ops_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_time(\"DmlExecutionProvider\",\"MemcpyFromHost\", path)\n",
    "get_time(\"DmlExecutionProvider\",\"MemcpyToHost\", path)\n",
    "get_time(\"DmlExecutionProvider\",None, path)\n",
    "get_time(\"CPUExecutionProvider\",None, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernelname(provider, op,filename):\n",
    "    ops = set()\n",
    "    with open(path, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        for i in range(len(data)):\n",
    "            if \"op_name\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            op_name = data[i][\"args\"][\"op_name\"]               \n",
    "            if \"provider\" in data[i][\"args\"]:\n",
    "                provider_name = data[i][\"args\"][\"provider\"]\n",
    "                if provider == provider_name:\n",
    "                    if op == op_name or op == None:\n",
    "                        print(data[i]['name'])\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_kernelname(\"CPUExecutionProvider\",\"Mul\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type(filename):\n",
    "    input_data_type = set()\n",
    "    output_data_type = set()\n",
    "    with open(filename, 'r') as f:\n",
    "        # load the contents of the file into a dictionary\n",
    "        data = json.load(f)\n",
    "        max_size = 0\n",
    "        for i in range(len(data)):\n",
    "            if \"provider\" not in data[i][\"args\"]:\n",
    "                continue\n",
    "            \n",
    "            inputs = data[i][\"args\"][\"input_type_shape\"]\n",
    "            outputs = data[i][\"args\"][\"output_type_shape\"]\n",
    "            for input in inputs:\n",
    "                for key in input.keys():\n",
    "                    input_data_type.add(key)\n",
    "            for output in outputs:\n",
    "                for key in output.keys():\n",
    "                    output_data_type.add(key)\n",
    "    print(input_data_type)\n",
    "    print(output_data_type)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\onnxruntime_profile__DG2_unet_1.5_olive.json'\n",
    "get_type(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze openvino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(r'..\\AIGC\\optimize\\unet_ov.xml')\n",
    "root = tree.getroot()\n",
    "for child in root:\n",
    "    if child.tag !=\"layers\":\n",
    "        continue\n",
    "    for subchild in child:\n",
    "        if \"FullyConnected\" == subchild.attrib[\"type\"]:\n",
    "            ports = subchild.find(\"input/port\")\n",
    "            if len(ports) == 2:\n",
    "                print(subchild.attrib[\"name\"])\n",
    "                break\n",
    "            # data_size = 1\n",
    "            # for value in ports.iter(\"dim\"):\n",
    "            #     data_size *= int(value.text)\n",
    "            # print(data_size)\n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert onnx memory file into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_file = r\"C:\\Users\\GAME\\Documents\\Project\\AIGC\\perf\\unet_mem_all.txt\"\n",
    "csv_file = mem_file.replace(\"txt\", 'csv')\n",
    "\n",
    "\n",
    "with open(mem_file,'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if \"peak_rss\" not in line:\n",
    "            continue\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pandasForSortingCSV\n",
    "def get_graph_info(file_csv,idx):\n",
    "    # assign dataset\n",
    "    csvData = pandasForSortingCSV.read_csv(file_csv)\n",
    "    print(csvData.groupby(csvData.columns[idx]).sum() )\n",
    "                                         \n",
    "    # sort data frame\n",
    "    # csvData.sort_values(csvData.columns[1], \n",
    "    #                     axis=0,\n",
    "    #                     inplace=True)                    \n",
    "    \n",
    "    # # displaying sorted data frame\n",
    "    # print(\"\\nAfter sorting:\")\n",
    "    # print(csvData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
